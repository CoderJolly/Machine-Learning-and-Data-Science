{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "\n",
    "import gym\n",
    "import plotting\n",
    "from lib.envs.cliff_walking import CliffWalkingEnv\n",
    "import itertools\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = CliffWalkingEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create a random epsilon-greedy policy\n",
    "\n",
    "def random_epsilon_greedy_policy(Q1, Q2, epsilon, state, nA):\n",
    "    A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "    best_action = np.argmax(Q1[state]+Q2[state])\n",
    "    A[best_action] += (1.0 - epsilon)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def double_q_learning(env, num_episodes, discount=1.0, alpha=0.5, epsilon=0.1, debug=False):\n",
    "    \n",
    "    Q1 = defaultdict(lambda: np.zeros(env.action_space.n, dtype=float))\n",
    "    Q2 = defaultdict(lambda: np.zeros(env.action_space.n, dtype=float))\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n, dtype=float))\n",
    "    \n",
    "    episode_lengths = defaultdict(float)\n",
    "    episode_rewards = defaultdict(float)\n",
    "    \n",
    "\n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        \n",
    "        if debug:\n",
    "            if i_episode % 100 == 0:\n",
    "                print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes))\n",
    "                \n",
    "        state = env.reset()\n",
    "        for t in itertools.count():\n",
    "            action_probs = random_epsilon_greedy_policy(Q1, Q2, epsilon, state, env.action_space.n)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            \n",
    "            next_state, reward, end, _ = env.step(action)\n",
    "            \n",
    "            if np.random.choice([0,1]) == 0:\n",
    "                Q1[state][action] += alpha * (reward + discount*Q2[next_state][np.argmax(Q1[next_state][:])] - Q1[state][action])\n",
    "            else:\n",
    "                Q2[state][action] += alpha * (reward + discount*Q1[next_state][np.argmax(Q2[next_state][:])] - Q2[state][action])\n",
    "            \n",
    "            episode_rewards[i_episode] += reward\n",
    "            episode_lengths[i_episode] = t\n",
    "            \n",
    "            Q[state][action] = (Q1[state][action] + Q2[state][action])/2\n",
    "            if end:\n",
    "                break\n",
    "            state = next_state\n",
    "            \n",
    "    return Q, episode_lengths, episode_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100/500.\n",
      "Episode 200/500.\n",
      "Episode 300/500.\n",
      "Episode 400/500.\n",
      "Episode 500/500.\n"
     ]
    }
   ],
   "source": [
    "Q, episode_lengths, episode_rewards = double_q_learning(env, 500, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>>,\n",
       "            {0: array([-86.25282999, -23.44744607, -67.90837437, -85.79483728]),\n",
       "             1: array([-82.84831388, -13.01172846, -83.37924278, -83.83748805]),\n",
       "             2: array([-22.707259  , -12.        , -27.93397581, -18.3378195 ]),\n",
       "             3: array([-14.16415677, -11.        , -38.10387981, -26.53612156]),\n",
       "             4: array([-22.10994281, -10.        , -25.54909169, -17.74410247]),\n",
       "             5: array([-20.01943094, -58.29430647,  -9.        , -43.71043444]),\n",
       "             6: array([-53.84787044, -24.23836684, -65.35721588, -66.69837795]),\n",
       "             7: array([-61.03635374, -58.99763827,  -7.10116709, -60.82662748]),\n",
       "             8: array([-53.6463428 , -54.50774581, -54.3417167 , -58.12313906]),\n",
       "             9: array([-47.41744092, -47.96204128, -42.73591552, -47.78883778]),\n",
       "             10: array([-40.58736277, -38.45627719, -38.64294958, -42.33035229]),\n",
       "             11: array([-32.491102  , -34.57036495, -32.72227465, -37.72419834]),\n",
       "             12: array([-42.5178219 , -15.        , -29.60956574, -28.17677653]),\n",
       "             13: array([-31.76997674, -14.        , -45.35541585, -23.52061371]),\n",
       "             14: array([-13.        , -48.72466025, -50.43491602, -52.35034579]),\n",
       "             15: array([-75.67025906, -14.5216618 , -79.98233225, -75.66984776]),\n",
       "             16: array([-74.32505037,  -9.03110931, -72.09420167, -73.17870298]),\n",
       "             17: array([-22.48331298,  -8.        , -39.48362317, -17.34197723]),\n",
       "             18: array([-68.19381616,  -7.        , -21.30380364, -40.08569754]),\n",
       "             19: array([-19.55064803, -25.91409623,  -6.        , -21.46125672]),\n",
       "             20: array([-52.91557324, -38.61268152, -49.79617546,  -8.16611205]),\n",
       "             21: array([-53.6680572 , -38.49237294,  -5.49946523, -52.01180221]),\n",
       "             22: array([-38.17007509, -29.19906455, -34.44189453, -36.94431455]),\n",
       "             23: array([-16.77714658, -15.15039635,  -2.0809187 , -28.23580933]),\n",
       "             24: array([-16.        , -43.99657678, -27.73670451, -21.87908248]),\n",
       "             25: array([-16.72222903, -86.14475247, -90.625     , -87.02038289]),\n",
       "             26: array([-27.1960263 , -83.96281234, -84.375     , -81.74357195]),\n",
       "             27: array([-76.68730238, -75.28681321, -81.25      , -73.61057883]),\n",
       "             28: array([-68.84005108, -44.1342461 , -71.875     , -72.01386502]),\n",
       "             29: array([-64.84347127,  -9.96515763, -62.5       , -63.50599489]),\n",
       "             30: array([-62.24719186,  -6.20274301, -87.5       , -53.11490657]),\n",
       "             31: array([-10.98588967,  -5.        , -99.51171875, -28.99059346]),\n",
       "             32: array([-36.91839676,  -4.        , -98.828125  , -15.76269448]),\n",
       "             33: array([-24.48653702,  -3.        , -92.1875    ,  -9.8734291 ]),\n",
       "             34: array([-41.86744962,  -2.        , -98.4375    , -14.65429688]),\n",
       "             35: array([-34.49356842,  -2.96907043,  -1.        ,  -4.19332893]),\n",
       "             36: array([-17.        , -99.51171875, -43.02810992, -20.18701866])})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

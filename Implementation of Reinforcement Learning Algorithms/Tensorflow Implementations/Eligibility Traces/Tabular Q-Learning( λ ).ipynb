{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "\n",
    "import gym\n",
    "import plotting\n",
    "from lib.envs.gridworld import GridworldEnv\n",
    "import itertools\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = GridworldEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create an initial epsilon soft policy\n",
    "def epsilon_greedy_policy(Q, epsilon, state, nA):\n",
    "    A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "    best_action = np.argmax(Q[state])\n",
    "    A[best_action] += (1.0 - epsilon)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def online_q_learning_lambda(env, num_episodes, discount=1.0, epsilon=0.1, alpha=0.5, lbda=0.9, debug=False):\n",
    "    \n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n, dtype=float))\n",
    "    \n",
    "    for i_episode in range(1, num_episodes+1):\n",
    "        \n",
    "        if debug:\n",
    "            if i_episode % 1000 == 0:\n",
    "                print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes))\n",
    "                \n",
    "        E = {key:np.zeros(4, dtype=int) for key in np.arange(16)}\n",
    "        state = env.reset()\n",
    "        action_probs = epsilon_greedy_policy(Q, epsilon, state, env.nA)\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        for t in itertools.count():\n",
    "            next_state, reward, end, _ = env.step(action)\n",
    "            \n",
    "            next_action_probs = epsilon_greedy_policy(Q, epsilon, next_state, env.nA)\n",
    "            next_action = np.random.choice(np.arange(len(next_action_probs)), p=next_action_probs)\n",
    "            \n",
    "            best_greedy_action = np.argmax(Q[next_state][:])\n",
    "            delta = reward + discount*Q[next_state][best_greedy_action] - Q[state][action]\n",
    "            E[state][action] += 1\n",
    "            \n",
    "            for s in E.keys():\n",
    "                for a in E[s]:\n",
    "                    Q[s][a] += alpha*delta*E[s][a]\n",
    "                    \n",
    "                    if next_action == best_greedy_action:\n",
    "                        E[s][a] = discount*lbda*E[s][a]\n",
    "                    else:\n",
    "                        E[s][a] = 0\n",
    "                    \n",
    "            if end:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "            action = next_action\n",
    "            \n",
    "    return Q    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000/10000.\n",
      "Episode 2000/10000.\n",
      "Episode 3000/10000.\n",
      "Episode 4000/10000.\n",
      "Episode 5000/10000.\n",
      "Episode 6000/10000.\n",
      "Episode 7000/10000.\n",
      "Episode 8000/10000.\n",
      "Episode 9000/10000.\n",
      "Episode 10000/10000.\n"
     ]
    }
   ],
   "source": [
    "Q = online_q_learning_lambda(env, num_episodes=10000, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function __main__.<lambda>>,\n",
       "            {0: array([ 0.,  0.,  0.,  0.]),\n",
       "             1: array([-1.   , -2.5  , -1.875,  0.   ]),\n",
       "             2: array([-2.49999952, -2.99983215, -2.        , -1.5       ]),\n",
       "             3: array([-2.99978638, -2.99218732, -2.        , -2.99999999]),\n",
       "             4: array([-1. , -2. , -1. , -1.5]),\n",
       "             5: array([-1.        , -2.99999988, -3.        , -2.9765625 ]),\n",
       "             6: array([-2.484375, -2.      , -2.625   , -2.8125  ]),\n",
       "             7: array([-3.        , -2.        , -1.        , -2.99981689]),\n",
       "             8: array([-2.        , -3.        , -4.        , -2.57666016]),\n",
       "             9: array([-2.       , -2.       , -2.3671875, -2.625    ]),\n",
       "             10: array([-2.99993892, -1.        , -1.        , -2.953125  ]),\n",
       "             11: array([-2., -1.,  0.,  0.]),\n",
       "             12: array([-3.        , -3.        , -3.99451293, -3.        ]),\n",
       "             13: array([-2.99964142, -2.        , -2.55859375, -2.953125  ]),\n",
       "             14: array([-2.      , -1.      , -2.      , -2.953125]),\n",
       "             15: array([ 0.,  0.,  0.,  0.])})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

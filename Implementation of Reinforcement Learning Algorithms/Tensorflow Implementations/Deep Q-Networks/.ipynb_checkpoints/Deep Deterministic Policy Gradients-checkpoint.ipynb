{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import psutil\n",
    "import tensorflow as tf\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "\n",
    "from lib import plotting\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-22 21:04:27,382] Making new env: Pendulum-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Pendulum-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReplayMemory():\n",
    "    \n",
    "    def __init__(self, init_size, max_size, batch_size):\n",
    "        self.replay_memory = []\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def append(self, transition):\n",
    "        self.replay_memory.append(transition)\n",
    "        \n",
    "    def pop(self):\n",
    "        return self.replay_memory.pop(0)\n",
    "    \n",
    "    def sample(self):\n",
    "        return random.sample(self.replay_memory, self.batch_size)\n",
    "    \n",
    "    def get_size(self):\n",
    "        return len(self.replay_memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UpdateTargetNetwork():\n",
    "    \n",
    "    def __init__(self, tau, estimator, target_estimator, scope):\n",
    "        self.scope = scope\n",
    "        self.tau = tau\n",
    "        with tf.variable_scope(scope):\n",
    "            e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator.scope)]\n",
    "            e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "\n",
    "            e2_params = [t for t in tf.trainable_variables() if t.name.startswith(target_estimator.scope)]\n",
    "            e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "        \n",
    "            self.update_ops = []\n",
    "            for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "                op = e2_v.assign((1-tau)*e2_v + tau*e1_v)\n",
    "                self.update_ops.append(op)\n",
    "                \n",
    "    def update(self, sess):\n",
    "        return sess.run(self.update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Actor():\n",
    "    \n",
    "    def __init__(self, tau, learning_rate, scope=\"actor\"):\n",
    "        self.scope = scope\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.action_bound = env.action_space.high\n",
    "        self.s_dim = env.observation_space.shape[0]\n",
    "        self.a_dim = env.action_space.shape[0]\n",
    "        with tf.variable_scope(scope):\n",
    "            self._build_model()\n",
    "            \n",
    "        \n",
    "    def _build_model(self):\n",
    "        self.X_pl = tf.placeholder(dtype=tf.float32, shape=[None, self.s_dim], name=\"X\")\n",
    "        self.y_pl = tf.placeholder(dtype=tf.float32, shape=[None], name=\"y\")\n",
    "        self.actions_pl = tf.placeholder(dtype=tf.int32, shape=[None, self.a_dim], name=\"actions\")\n",
    "        self.action_gradient = tf.placeholder(dtype=tf.float32, shape=[None, self.a_dim], name=\"action_gradients\")\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "        \n",
    "        #3 Fully Connected Layers\n",
    "        fc1 = tf.contrib.layers.fully_connected(self.X_pl, num_outputs=400, activation_fn=tf.nn.relu)\n",
    "        fc2 = tf.contrib.layers.fully_connected(fc1, num_outputs=300, activation_fn=tf.nn.relu)\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc2,\n",
    "                                                             num_outputs=env.action_space.shape[0],\n",
    "                                                             activation_fn=tf.nn.tanh)\n",
    "\n",
    "        \n",
    "        self.scaled_predictions = tf.multiply(self.predictions, self.action_bound)\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "        \n",
    "        self.losses = -tf.log(self.action_predictions) * self.y_pl\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "        \n",
    "        self.optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "        \n",
    "        self.network_params = tf.trainable_variables()\n",
    "        self.actor_gradients = tf.gradients(\n",
    "            self.scaled_predictions, self.network_params, -self.action_gradient)\n",
    "    \n",
    "    def predict(self, sess, state):\n",
    "        return sess.run(self.scaled_predictions, feed_dict={ self.X_pl: state })\n",
    "    \n",
    "    def update_gradient(self, sess, s, a_grads):\n",
    "        \n",
    "        feed_dict = { self.X_pl:s, self.action_gradient:a_grads }\n",
    "        return sess.run(self.actor_gradients, feed_dict=feed_dict)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Critic():\n",
    "    \n",
    "    def __init__(self, tau, learning_rate, scope=\"critic\"):\n",
    "        self.scope = scope\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.s_dim = env.observation_space.shape[0]\n",
    "        self.a_dim = env.action_space.shape[0]\n",
    "        with tf.variable_scope(scope):\n",
    "            self._build_model()\n",
    "            \n",
    "        \n",
    "    def _build_model(self):\n",
    "        self.X_pl = tf.placeholder(dtype=tf.float32, shape=[None, self.s_dim], name=\"X\")\n",
    "        self.actions_pl = tf.placeholder(dtype=tf.float32, shape=[None, self.a_dim], name=\"actions\")\n",
    "        self.y_pl = tf.placeholder(dtype=tf.float32, shape=[None], name=\"y\")\n",
    "        \n",
    "        #2 Fully Connected Layers with temp layers\n",
    "        fc1 = tf.contrib.layers.fully_connected(self.X_pl, 400, activation_fn=tf.nn.relu)\n",
    "        fc2_1 = tf.contrib.layers.fully_connected(fc1, 300)\n",
    "        fc2_2 = tf.contrib.layers.fully_connected(self.actions_pl, 300)\n",
    "        \n",
    "        fc2_1_w = [t for t in tf.trainable_variables() if t.name == 'critic/fully_connected_1/weights:0'][0]\n",
    "        fc2_2_w = [t for t in tf.trainable_variables() if t.name == 'critic/fully_connected_2/weights:0'][0]\n",
    "        fc2_2_b = [t for t in tf.trainable_variables() if t.name == 'critic/fully_connected_2/biases:0'][0]\n",
    "        \n",
    "        out = tf.matmul(fc1, fc2_1_w) + tf.matmul(self.actions_pl, fc2_2_w) + fc2_2_b\n",
    "        self.action_value_predictions = tf.contrib.layers.fully_connected(out, 1, activation_fn=tf.nn.relu)\n",
    "        \n",
    "        self.losses = tf.squared_difference(self.action_value_predictions, self.y_pl)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "        \n",
    "        self.optimizer = tf.train.RMSPropOptimizer(self.learning_rate)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "        \n",
    "        self.action_gradients = tf.gradients(self.action_value_predictions, self.actions_pl)\n",
    "    \n",
    "    def predict(self, sess, s, a):\n",
    "        return sess.run(self.action_value_predictions, feed_dict={ self.X_pl: s, self.actions_pl:a })\n",
    "    \n",
    "    def update(self, sess, s, a, y):\n",
    "        \n",
    "        feed_dict = { self.X_pl:s, self.y_pl:y, self.actions_pl:a }\n",
    "        _, loss = sess.run([self.train_op, self.loss], feed_dict=feed_dict)\n",
    "        return loss\n",
    "    \n",
    "    def get_action_gradients(self, sess, s, a):\n",
    "        return sess.run(self.action_gradients, feed_dict={ self.X_pl:s, self.actions_pl:a})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deep_policy_gradients(sess,\n",
    "                    env,\n",
    "                    actor,\n",
    "                    critic,\n",
    "                    actor_target,\n",
    "                    critic_target,\n",
    "                    num_episodes,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=50):\n",
    "    \n",
    "    episode_loss = []\n",
    "    Transition = namedtuple(\"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        \n",
    "    #Initilaize Epsilons\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "    \n",
    "    #Initialize Replay Memory\n",
    "    replay_memory = ReplayMemory(replay_memory_init_size, replay_memory_size, batch_size)\n",
    "    print(\"populating replay memory\")\n",
    "    state = env.reset()\n",
    "    for i in range(replay_memory_init_size):\n",
    "        action = actor.predict(sess, np.reshape(state, (1,3))) + 1./1. + i\n",
    "        next_state, reward, done, _ = env.step(action[0])\n",
    "        replay_memory.append(Transition(np.reshape(state, (actor.s_dim,)), np.reshape(action, (actor.a_dim,)),\n",
    "                                        reward, np.reshape(next_state, (actor.s_dim,)), done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "        else:\n",
    "            state = next_state\n",
    "    \n",
    "    \n",
    "    #Update target actor-critic\n",
    "    actor_target_updater = UpdateTargetNetwork(tau, actor, actor_target, scope=\"update_actor_target\")\n",
    "    actor_target_updater.update(sess)\n",
    "    \n",
    "    critic_target_updater = UpdateTargetNetwork(tau, critic, critic_target, scope=\"update_critic_target\")\n",
    "    critic_target_updater.update(sess)\n",
    "    \n",
    "    total_reward_gained = 0\n",
    "    \n",
    "    #Start learning\n",
    "    state = env.reset()\n",
    "    for i_episode in range(num_episodes):\n",
    "        \n",
    "        #Add exploration noise\n",
    "        print(\"Get next state and rewards\")\n",
    "        action = actor.predict(sess, np.reshape(state, (1,3))) + 1./1. + i_episode\n",
    "        next_state, reward, done, _ = env.step(action[0])\n",
    "        \n",
    "        #Add transition to memory\n",
    "        print(\"Add to replay memory\")\n",
    "        replay_memory.append(Transition(np.reshape(state, (actor.s_dim,)), np.reshape(action, (actor.a_dim,)),\n",
    "                                        reward, np.reshape(next_state, (actor.s_dim,)), done))\n",
    "        \n",
    "        #Pop the first transition if the replay memory gets full\n",
    "        if replay_memory.get_size() >= replay_memory_size:\n",
    "            replay_memory.pop()\n",
    "            \n",
    "        #Randomly sample from replay memory\n",
    "        print(\"Sample from the memory\")\n",
    "        samples = replay_memory.sample()\n",
    "        states_batch, actions_batch, rewards_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "        \n",
    "        #Update the critic with the targets\n",
    "        print(\"Update critic with target\")\n",
    "        q_values_next = critic_target.predict(sess, next_states_batch, actor_target.predict(sess, next_states_batch))\n",
    "        targets_batch = rewards_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1)\n",
    "        loss = critic.update(sess, states_batch, actions_batch, targets_batch)\n",
    "        \n",
    "        #Update the actor with the gradients\n",
    "        print(\"Update actor gradient\")\n",
    "        predicted_actions_batch = actor.predict(sess, states_batch)\n",
    "        action_gradients_batch = critic.get_action_gradients(sess, states_batch, predicted_actions_batch)\n",
    "        actor.update_gradient(sess, states_batch, action_gradients_batch[0])\n",
    "    \n",
    "        #Update target networks\n",
    "        print(\"Update target networks\")\n",
    "        actor_target_updater.update(sess)\n",
    "        critic_target_updater.update(sess)\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "        state = next_state\n",
    "        total_reward_gained += reward\n",
    "        \n",
    "        episode_loss.append(loss)\n",
    "        print loss\n",
    "        s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "populating replay memory\n",
      "Get next state and rewards\n",
      "Add to replay memory\n",
      "Sample from the memory\n",
      "Update critic with target\n",
      "Update actor gradient\n",
      "[[ 0.13966396]\n",
      " [-0.43929243]\n",
      " [-0.54813761]\n",
      " [ 0.12418531]\n",
      " [-0.4349741 ]\n",
      " [-0.54281658]\n",
      " [-0.25805441]\n",
      " [-0.35066798]\n",
      " [ 0.04961627]\n",
      " [-0.50596958]\n",
      " [-0.14419   ]\n",
      " [-0.36073563]\n",
      " [-0.70022857]\n",
      " [-0.22213225]\n",
      " [-0.69198722]\n",
      " [ 0.19086984]\n",
      " [-0.47850454]\n",
      " [ 0.05552344]\n",
      " [-0.08996425]\n",
      " [-0.69432509]\n",
      " [ 0.02476518]\n",
      " [-0.58344424]\n",
      " [-0.39837417]\n",
      " [-0.61992568]\n",
      " [-0.20832171]\n",
      " [-0.36708239]\n",
      " [-0.36909261]\n",
      " [-0.65425164]\n",
      " [-0.36145258]\n",
      " [-0.20287432]\n",
      " [-0.71347344]\n",
      " [-0.44395462]]\n",
      "#################################\n",
      "[array([[ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.12101294],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.12101294],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.12101294],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [-0.12101294],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ],\n",
      "       [ 0.        ]], dtype=float32)]\n",
      "Update target networks\n",
      "70.2516\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "global name 's' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-4d1ea15450e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m                         \u001b[0mepsilon_decay_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m                         \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                         batch_size=32)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-56-302230bf119a>\u001b[0m in \u001b[0;36mdeep_policy_gradients\u001b[0;34m(sess, env, actor, critic, actor_target, critic_target, num_episodes, replay_memory_size, replay_memory_init_size, update_target_estimator_every, discount_factor, epsilon_start, epsilon_end, epsilon_decay_steps, batch_size, record_video_every)\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mepisode_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: global name 's' is not defined"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Create a glboal step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "tau=0.001\n",
    "actor_learning_rate=0.0001\n",
    "critic_learning_rate=0.001\n",
    "\n",
    "\n",
    "#Initilaize actor-critic\n",
    "actor = Actor(tau, actor_learning_rate, scope=\"actor\")\n",
    "critic = Critic(tau, critic_learning_rate, scope=\"critic\")\n",
    "\n",
    "#Initialize target actor-critics\n",
    "actor_target = Actor(tau, actor_learning_rate, scope=\"actor_target\")\n",
    "critic_target = Critic(tau, critic_learning_rate, scope=\"critic_target\")\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    deep_policy_gradients(sess,\n",
    "                        env,\n",
    "                        actor,\n",
    "                        critic,\n",
    "                        actor_target,\n",
    "                        critic_target,\n",
    "                        num_episodes=10000,\n",
    "                        replay_memory_size=500000,\n",
    "                        replay_memory_init_size=50000,\n",
    "                        update_target_estimator_every=10000,\n",
    "                        epsilon_start=1.0,\n",
    "                        epsilon_end=0.1,\n",
    "                        epsilon_decay_steps=500000,\n",
    "                        discount_factor=0.99,\n",
    "                        batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

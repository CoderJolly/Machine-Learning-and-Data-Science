{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "import itertools\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import psutil\n",
    "import tensorflow as tf\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\")\n",
    "\n",
    "from lib import plotting\n",
    "from collections import deque, namedtuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-08-18 22:43:12,830] Making new env: Breakout-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Breakout-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action space size: 4\n",
      "['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n",
      "Observation space shape: (210, 160, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"Action space size: {}\".format(env.action_space.n))\n",
    "print(env.unwrapped.get_action_meanings())\n",
    "\n",
    "observation = env.reset()\n",
    "print(\"Observation space shape: {}\".format(observation.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n20zMMtS4TwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c4e5510>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAANEAAAD8CAYAAADpCEEHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n+0zMIkDtgwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x103f342d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "[env.step(2) for x in range(1)]\n",
    "plt.figure()\n",
    "plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "env.render(close=True)\n",
    "\n",
    "env.step(1)\n",
    "env.render(close=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1055c9650>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10c4d5e50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cropped image\n",
    "plt.imshow(observation[34:-16,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Atari Actions: 0 (noop), 1 (fire), 2 (left) and 3 (right) are valid actions\n",
    "VALID_ACTIONS = [0,1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class StateProcessor():\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        with tf.variable_scope(\"state_processor\"):\n",
    "            self.input_state = tf.placeholder(dtype=tf.uint8, shape=[210, 160, 3])\n",
    "            self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
    "            self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n",
    "            self.output = tf.image.resize_images(self.output, [84, 84], method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
    "            self.output = tf.squeeze(self.output)\n",
    "            \n",
    "    def process(self, sess, state):\n",
    "        return sess.run(self.output, feed_dict={ self.input_state:state })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Estimator():\n",
    "  \n",
    "    def __init__(self, scope=\"estimator\", summaries_dir=None):\n",
    "        self.scope = scope\n",
    "        # Writes Tensorboard summaries to disk\n",
    "        self.summary_writer = None\n",
    "        with tf.variable_scope(scope):\n",
    "            # Build the graph\n",
    "            self._build_model()\n",
    "            if summaries_dir:\n",
    "                summary_dir = os.path.join(summaries_dir, \"summaries_{}\".format(scope))\n",
    "                if not os.path.exists(summary_dir):\n",
    "                    os.makedirs(summary_dir)\n",
    "                self.summary_writer = tf.summary.FileWriter(summary_dir)\n",
    "\n",
    "    def _build_model(self):\n",
    "      \n",
    "        # Placeholders for our input\n",
    "        # Our input are 4 RGB frames of shape 160, 160 each\n",
    "        self.X_pl = tf.placeholder(shape=[None, 84, 84, 4], dtype=tf.uint8, name=\"X\")\n",
    "        self.y_pl = tf.placeholder(shape=[None], dtype=tf.float32, name=\"y\")\n",
    "        self.actions_pl = tf.placeholder(shape=[None], dtype=tf.int32, name=\"actions\")\n",
    "\n",
    "        X = tf.to_float(self.X_pl) / 255.0\n",
    "        batch_size = tf.shape(self.X_pl)[0]\n",
    "\n",
    "        # Three convolutional layers\n",
    "        conv1 = tf.contrib.layers.conv2d(\n",
    "            X, 32, 8, 4, activation_fn=tf.nn.relu)\n",
    "        conv2 = tf.contrib.layers.conv2d(\n",
    "            conv1, 64, 4, 2, activation_fn=tf.nn.relu)\n",
    "        conv3 = tf.contrib.layers.conv2d(\n",
    "            conv2, 64, 3, 1, activation_fn=tf.nn.relu)\n",
    "\n",
    "        # Fully connected layers\n",
    "        flattened = tf.contrib.layers.flatten(conv3)\n",
    "        fc1 = tf.contrib.layers.fully_connected(flattened, 512)\n",
    "        self.predictions = tf.contrib.layers.fully_connected(fc1, len(VALID_ACTIONS))\n",
    "\n",
    "        # Get the predictions for the chosen actions only\n",
    "        gather_indices = tf.range(batch_size) * tf.shape(self.predictions)[1] + self.actions_pl\n",
    "        self.action_predictions = tf.gather(tf.reshape(self.predictions, [-1]), gather_indices)\n",
    "\n",
    "        # Calculate the loss\n",
    "        self.losses = tf.squared_difference(self.y_pl, self.action_predictions)\n",
    "        self.loss = tf.reduce_mean(self.losses)\n",
    "\n",
    "        # Optimizer Parameters from original paper\n",
    "        self.optimizer = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6)\n",
    "        self.train_op = self.optimizer.minimize(self.loss, global_step=tf.contrib.framework.get_global_step())\n",
    "\n",
    "        # Summaries for Tensorboard\n",
    "        self.summaries = tf.summary.merge([\n",
    "            tf.summary.scalar(\"loss\", self.loss),\n",
    "            tf.summary.histogram(\"loss_hist\", self.losses),\n",
    "            tf.summary.histogram(\"q_values_hist\", self.predictions),\n",
    "            tf.summary.scalar(\"max_q_value\", tf.reduce_max(self.predictions))\n",
    "        ])\n",
    "\n",
    "    def predict(self, sess, s):\n",
    "        \n",
    "        return sess.run(self.predictions, { self.X_pl: s })\n",
    "\n",
    "    def update(self, sess, s, a, y):\n",
    "        \n",
    "        feed_dict = { self.X_pl: s, self.y_pl: y, self.actions_pl: a }\n",
    "        summaries, global_step, _, loss = sess.run(\n",
    "            [self.summaries, tf.contrib.framework.get_global_step(), self.train_op, self.loss],\n",
    "            feed_dict)\n",
    "        if self.summary_writer:\n",
    "            self.summary_writer.add_summary(summaries, global_step)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "\n",
    "e = Estimator(scope=\"test\")\n",
    "sp = StateProcessor()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    observation = env.reset()\n",
    "    \n",
    "    observation_p = sp.process(sess, observation)\n",
    "    observation = np.stack([observation_p] * 4, axis=2)\n",
    "    observations = np.array([observation] * 2)\n",
    "    \n",
    "    y = np.array([10.0, 10.0])\n",
    "    a = np.array([1, 3])\n",
    "    print(e.update(sess, observations, a, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelParametersCopier():\n",
    "    \n",
    "    def __init__(self, estimator1, estimator2):\n",
    "        \n",
    "        e1_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator1.scope)]\n",
    "        e1_params = sorted(e1_params, key=lambda v: v.name)\n",
    "        \n",
    "        e2_params = [t for t in tf.trainable_variables() if t.name.startswith(estimator2.scope)]\n",
    "        e2_params = sorted(e2_params, key=lambda v: v.name)\n",
    "        \n",
    "        self.update_ops = []\n",
    "        for e1_v, e2_v in zip(e1_params, e2_params):\n",
    "            op = e2_v.assign(e1_v)\n",
    "            self.update_ops.append(op)\n",
    "            \n",
    "    def make(self, sess):\n",
    "        return sess.run(self.update_ops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_epsilon_greedy_policy(estimator, nA):\n",
    "    \n",
    "    \n",
    "    def policy_fn(sess, observation, epsilon):\n",
    "        A = np.ones(nA, dtype=float) * epsilon/nA\n",
    "        q_values = estimator.predict(sess, np.expand_dims(observation, 0))[0]\n",
    "        best_action = np.argmax(q_values)\n",
    "        A[best_action] += (1 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator_fn(stats):\n",
    "    \n",
    "    yield total_t, plotting.EpisodeStats(\n",
    "            episode_lengths=stats.episode_lengths[:i_episode+1],\n",
    "            episode_rewards=stats.episode_rewards[:i_episode+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deep_q_learning(sess,\n",
    "                    env,\n",
    "                    q_estimator,\n",
    "                    target_estimator,\n",
    "                    state_processor,\n",
    "                    num_episodes,\n",
    "                    experiment_dir,\n",
    "                    replay_memory_size=500000,\n",
    "                    replay_memory_init_size=50000,\n",
    "                    update_target_estimator_every=10000,\n",
    "                    discount_factor=0.99,\n",
    "                    epsilon_start=1.0,\n",
    "                    epsilon_end=0.1,\n",
    "                    epsilon_decay_steps=500000,\n",
    "                    batch_size=32,\n",
    "                    record_video_every=50):\n",
    "    \n",
    "    \n",
    "    Transition = namedtuple(\"Transition\", ['state', 'action', 'reward', 'next_state', 'done'])\n",
    "    \n",
    "    # The replay memory\n",
    "    replay_memory = []\n",
    "    \n",
    "    # Make model copier object\n",
    "    estimator_copy = ModelParametersCopier(q_estimator, target_estimator)\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))\n",
    "    \n",
    "    # For 'system/' summaries, usefull to check if currrent process looks healthy\n",
    "    current_process = psutil.Process()\n",
    "\n",
    "    # Create directories for checkpoints and summaries\n",
    "    checkpoint_dir = os.path.join(experiment_dir, \"checkpoints\")\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, \"model\")\n",
    "    monitor_path = os.path.join(experiment_dir, \"monitor\")\n",
    "    \n",
    "    if not os.path.exists(checkpoint_dir):\n",
    "        os.makedirs(checkpoint_dir)\n",
    "    if not os.path.exists(monitor_path):\n",
    "        os.makedirs(monitor_path)\n",
    "        \n",
    "    saver = tf.train.Saver()\n",
    "    \n",
    "    # Load a previous checkpoint if we find one\n",
    "    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    if latest_checkpoint:\n",
    "        print(\"Loading model checkpoint {}...\\n\".format(latest_checkpoint))\n",
    "        saver.restore(sess, latest_checkpoint)\n",
    "    \n",
    "    # Get the current time step\n",
    "    total_t = sess.run(tf.contrib.framework.get_global_step())\n",
    "    \n",
    "    # The epsilon decay schedule\n",
    "    epsilons = np.linspace(epsilon_start, epsilon_end, epsilon_decay_steps)\n",
    "    \n",
    "    policy = make_epsilon_greedy_policy(q_estimator, len(VALID_ACTIONS))\n",
    "    \n",
    "    #Initializing the replay memory\n",
    "    print(\"populating replay memory\")\n",
    "    state = env.reset()\n",
    "    state = state_processor.process(sess, state)\n",
    "    state = np.stack([state]*4, axis=2)\n",
    "    for i in range(replay_memory_init_size):\n",
    "        action_probs = policy(sess, state, epsilons[min(total_t, epsilon_decay_steps-1)])\n",
    "        action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "        \n",
    "        next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "        next_state = state_processor.process(sess, next_state)\n",
    "        next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "        replay_memory.append(Transition(state, action, reward, next_state, done))\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "            state = state_processor.process(sess, state)\n",
    "            state = np.stack([state]*4, axis=2)\n",
    "        else:\n",
    "            state = next_state\n",
    "            \n",
    "            \n",
    "    env = Monitor(env, directory=monitor_path, video_callable=lambda count: count % record_video_every == 0, resume=True)\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        \n",
    "        saver.save(tf.get_default_session(), checkpoint_path)\n",
    "        \n",
    "        state = env.reset()\n",
    "        state = state_processor.process(state)\n",
    "        state = np.stack([state]*4, axis=2)\n",
    "        loss = None\n",
    "        \n",
    "        for t in itertools.count():\n",
    "            \n",
    "            # Maybe update the target estimator\n",
    "            if total_t % update_target_estimator_every == 0:\n",
    "                estimator_copy.make(sess)\n",
    "                print(\"\\nCopied model parameters to target network.\")\n",
    "\n",
    "            # Print out which step we're on, useful for debugging.\n",
    "            print(\"\\rStep {} ({}) @ Episode {}/{}, loss: {}\".format(\n",
    "                    t, total_t, i_episode + 1, num_episodes, loss))\n",
    "            sys.stdout.flush()\n",
    "            \n",
    "            epsilon = epsilons[min(total_t, epsilon_decay_steps-1)]\n",
    "            action_probs = policy(sess, state, epsilon)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(VALID_ACTIONS[action])\n",
    "            next_state = state_processor.process(sess, next_state)\n",
    "            next_state = np.append(state[:,:,1:], np.expand_dims(next_state, 2), axis=2)\n",
    "            \n",
    "            # If our replay memory is full, pop the first element\n",
    "            if len(replay_memory) == replay_memory_size:\n",
    "                replay_memory.pop(0)\n",
    "\n",
    "            # Save transition to replay memory\n",
    "            replay_memory.append(Transition(state, action, reward, next_state, done))   \n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t \n",
    "            \n",
    "            samples = random.sample(replay_memory, batch_size)\n",
    "            states_batch, action_batch, reward_batch, next_states_batch, done_batch = map(np.array, zip(*samples))\n",
    "            \n",
    "            q_values_next = target_estimator.predict(sess, next_states_batch)\n",
    "            targets_batch = reward_batch + np.invert(done_batch).astype(np.float32) * discount_factor * np.amax(q_values_next, axis=1)\n",
    "            \n",
    "            loss = q_estimator.update(sess, states_batch, action_batch, targets_batch)\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            state = next_state\n",
    "            total_t += 1\n",
    "            \n",
    "        # Add summaries to tensorboard\n",
    "        episode_summary = tf.Summary()\n",
    "        episode_summary.value.add(simple_value=epsilon, tag=\"episode/epsilon\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_rewards[i_episode], tag=\"episode/reward\")\n",
    "        episode_summary.value.add(simple_value=stats.episode_lengths[i_episode], tag=\"episode/length\")\n",
    "        episode_summary.value.add(simple_value=current_process.cpu_percent(), tag=\"system/cpu_usage_percent\")\n",
    "        episode_summary.value.add(simple_value=current_process.memory_percent(memtype=\"vms\"), tag=\"system/v_memeory_usage_percent\")\n",
    "        q_estimator.summary_writer.add_summary(episode_summary, i_episode)\n",
    "        q_estimator.summary_writer.flush()\n",
    "        \n",
    "        \n",
    "        generator_fn(stats)\n",
    "        \n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "populating replay memory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-44cbb4bea53b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m                                     \u001b[0mepsilon_decay_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                                     \u001b[0mdiscount_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.99\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                                     batch_size=32):\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEpisode Reward: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-38c2ba054b94>\u001b[0m in \u001b[0;36mdeep_q_learning\u001b[0;34m(sess, env, q_estimator, target_estimator, state_processor, num_episodes, experiment_dir, replay_memory_size, replay_memory_init_size, update_target_estimator_every, discount_factor, epsilon_start, epsilon_end, epsilon_decay_steps, batch_size, record_video_every)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVALID_ACTIONS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mreplay_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTransition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-16-437833085b99>\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, sess, state)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_state\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/adityavyas/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adityavyas/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adityavyas/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/adityavyas/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/adityavyas/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Where we save our checkpoints and graphs\n",
    "experiment_dir = os.path.abspath(\"./experiments/{}\".format(env.spec.id))\n",
    "\n",
    "# Create a glboal step variable\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "    \n",
    "# Create estimators\n",
    "q_estimator = Estimator(scope=\"q_estimator\", summaries_dir=experiment_dir)\n",
    "target_estimator = Estimator(scope=\"target_q\")\n",
    "\n",
    "# State processor\n",
    "state_processor = StateProcessor()\n",
    "\n",
    "# Run it!\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for t, stats in deep_q_learning(sess,\n",
    "                                    env,\n",
    "                                    q_estimator=q_estimator,\n",
    "                                    target_estimator=target_estimator,\n",
    "                                    state_processor=state_processor,\n",
    "                                    experiment_dir=experiment_dir,\n",
    "                                    num_episodes=10000,\n",
    "                                    replay_memory_size=500000,\n",
    "                                    replay_memory_init_size=50000,\n",
    "                                    update_target_estimator_every=10000,\n",
    "                                    epsilon_start=1.0,\n",
    "                                    epsilon_end=0.1,\n",
    "                                    epsilon_decay_steps=500000,\n",
    "                                    discount_factor=0.99,\n",
    "                                    batch_size=32):\n",
    "\n",
    "        print(\"\\nEpisode Reward: {}\".format(stats.episode_rewards[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "from datetime import datetime, timedelta, date\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import string\n",
    "from glob import glob\n",
    "from numpy import nanmean\n",
    "from functools import partial\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GroupKFold, RandomizedSearchCV, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import lightgbm as lgb\n",
    "import scipy as sp\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from keras.applications.densenet import preprocess_input, DenseNet121\n",
    "from keras.models import Model\n",
    "from keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\n",
    "import keras.backend as K\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer, cohen_kappa_score, log_loss, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "\n",
    "# For text processing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.layers import Input, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# stop_words = []\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "warnings.filterwarnings('ignore')\n",
    "punctuation = string.punctuation\n",
    "\n",
    "pd.set_option('max_columns', 50)\n",
    "pd.options.mode.chained_assignment = None\n",
    "pp = pprint.PrettyPrinter(indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_orig = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv')\n",
    "test_orig = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d5823356caa8c49a4f608c26df447c7314bc003b"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "e8ea3930e4fb43649e8a83b23bf5576a181c698a"
   },
   "outputs": [],
   "source": [
    "def get_description_sentiment(df, df_type = \"train\"):\n",
    "    doc_sent_mag = []\n",
    "    doc_sent_score = []\n",
    "    nf_count = 0\n",
    "    for pet_id in df.PetID.values:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/' + df_type + '_sentiment/' + pet_id + '.json', 'r') as f:\n",
    "                sentiment = json.load(f)\n",
    "            doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "            doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            doc_sent_mag.append(-1)\n",
    "            doc_sent_score.append(-1)\n",
    "            \n",
    "    df['doc_sent_mag'] = doc_sent_mag\n",
    "    df['doc_sent_score'] = doc_sent_score\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "a53fa39250eeecd2571e9639dc902a818804bb6d"
   },
   "outputs": [],
   "source": [
    "def get_image_metadata(df, df_type = \"train\"):\n",
    "    vertex_xs = []\n",
    "    vertex_ys = []\n",
    "    bounding_confidences = []\n",
    "    bounding_importance_fracs = []\n",
    "    dominant_blues = []\n",
    "    dominant_greens = []\n",
    "    dominant_reds = []\n",
    "    dominant_pixel_fracs = []\n",
    "    dominant_scores = []\n",
    "    label_scores = []\n",
    "    nf_count = 0\n",
    "    nl_count = 0\n",
    "    \n",
    "    pets_metadata = {}\n",
    "    for file_name in os.listdir('../input/petfinder-adoption-prediction/' + df_type + '_metadata/'):\n",
    "        pet_id = file_name.split('-')[0]\n",
    "        if pet_id not in pets_metadata:\n",
    "            pets_metadata[pet_id] = []\n",
    "        pets_metadata[pet_id].append(file_name)\n",
    "    \n",
    "    for pet_id in df.PetID.values:\n",
    "        if pet_id in pets_metadata:\n",
    "            pet_id_metadata_files = pets_metadata[pet_id]\n",
    "            \n",
    "            temp_vertex_xs = []\n",
    "            temp_vertex_ys = []\n",
    "            temp_bounding_confidences = []\n",
    "            temp_bounding_importance_fracs = []\n",
    "            temp_dominant_blues = []\n",
    "            temp_dominant_greens = []\n",
    "            temp_dominant_reds = []\n",
    "            temp_dominant_pixel_fracs = []\n",
    "            temp_dominant_scores = []\n",
    "            temp_label_scores = []\n",
    "            for file in pet_id_metadata_files:\n",
    "                with open('../input/petfinder-adoption-prediction/' + df_type + '_metadata/' + file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                try:\n",
    "                    vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "                    temp_vertex_xs.append(vertex_x)\n",
    "                except:\n",
    "                    temp_vertex_xs.append(-1)\n",
    "                \n",
    "                try:\n",
    "                    vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "                    temp_vertex_ys.append(vertex_y)\n",
    "                except:\n",
    "                    temp_vertex_ys.append(-1)\n",
    "                \n",
    "                try:\n",
    "                    bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "                    temp_bounding_confidences.append(bounding_confidence)\n",
    "                except:\n",
    "                    temp_bounding_confidences.append(-1)\n",
    "                \n",
    "                try:\n",
    "                    bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "                    temp_bounding_importance_fracs.append(bounding_importance_frac)\n",
    "                except:\n",
    "                    temp_bounding_importance_fracs.append(-1)\n",
    "                \n",
    "                try:\n",
    "                    dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n",
    "                    temp_dominant_blues.append(dominant_blue)\n",
    "                except:\n",
    "                    temp_dominant_blues.append(-1)\n",
    "                \n",
    "                try:\n",
    "                    dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n",
    "                    temp_dominant_greens.append(dominant_green)\n",
    "                except:\n",
    "                    temp_dominant_greens.append(-1)\n",
    "                \n",
    "                try:\n",
    "                    dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n",
    "                    temp_dominant_reds.append(dominant_red)\n",
    "                except:\n",
    "                    temp_dominant_reds.append(-1)\n",
    "                \n",
    "                try:\n",
    "                    dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "                    temp_dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "                except:\n",
    "                    temp_dominant_pixel_fracs.append(-1)\n",
    "                \n",
    "                try:\n",
    "                    dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "                    temp_dominant_scores.append(dominant_score)\n",
    "                except:\n",
    "                    temp_dominant_scores.append(-1)\n",
    "\n",
    "                if data.get('labelAnnotations'):\n",
    "                    label_score = data['labelAnnotations'][0]['score']\n",
    "                    temp_label_scores.append(label_score)\n",
    "                else:\n",
    "                    temp_label_scores.append(-1)\n",
    "            \n",
    "            vertex_xs.append(np.mean(temp_vertex_xs))\n",
    "            vertex_ys.append(np.mean(temp_vertex_ys))\n",
    "            bounding_confidences.append(np.mean(temp_bounding_confidences))\n",
    "            bounding_importance_fracs.append(np.mean(temp_bounding_importance_fracs))\n",
    "            dominant_blues.append(np.mean(temp_dominant_blues))\n",
    "            dominant_greens.append(np.mean(temp_dominant_greens))\n",
    "            dominant_reds.append(np.mean(temp_dominant_reds))\n",
    "            dominant_pixel_fracs.append(np.mean(temp_dominant_pixel_fracs))\n",
    "            dominant_scores.append(np.mean(temp_dominant_scores))\n",
    "            label_scores.append(np.mean(temp_label_scores))\n",
    "        else:\n",
    "            nf_count += 1\n",
    "            vertex_xs.append(-1)\n",
    "            vertex_ys.append(-1)\n",
    "            bounding_confidences.append(-1)\n",
    "            bounding_importance_fracs.append(-1)\n",
    "            dominant_blues.append(-1)\n",
    "            dominant_greens.append(-1)\n",
    "            dominant_reds.append(-1)\n",
    "            dominant_pixel_fracs.append(-1)\n",
    "            dominant_scores.append(-1)\n",
    "            label_scores.append(-1)\n",
    "            \n",
    "    df.loc[:, 'vertex_x'] = vertex_xs\n",
    "    df.loc[:, 'vertex_y'] = vertex_ys\n",
    "    df.loc[:, 'bounding_confidence'] = bounding_confidences\n",
    "    df.loc[:, 'bounding_importance'] = bounding_importance_fracs\n",
    "    df.loc[:, 'dominant_blue'] = dominant_blues\n",
    "    df.loc[:, 'dominant_green'] = dominant_greens\n",
    "    df.loc[:, 'dominant_red'] = dominant_reds\n",
    "    df.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\n",
    "    df.loc[:, 'dominant_score'] = dominant_scores\n",
    "    df.loc[:, 'label_score'] = label_scores\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "24300b30525748ad1ac15dbda50c56f44c11a9ea"
   },
   "outputs": [],
   "source": [
    "# Reference - https://www.kaggle.com/shivamb/extensive-text-data-feature-engineering\n",
    "\n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 95000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 1257 # max number of words in a description to use\n",
    "\n",
    "pos_dic = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "mispell_dict = {\"aren't\" : \"are not\",\n",
    "                \"can't\" : \"cannot\",\n",
    "                \"couldn't\" : \"could not\",\n",
    "                \"didn't\" : \"did not\",\n",
    "                \"doesn't\" : \"does not\",\n",
    "                \"don't\" : \"do not\",\n",
    "                \"hadn't\" : \"had not\",\n",
    "                \"hasn't\" : \"has not\",\n",
    "                \"haven't\" : \"have not\",\n",
    "                \"he'd\" : \"he would\",\n",
    "                \"he'll\" : \"he will\",\n",
    "                \"he's\" : \"he is\",\n",
    "                \"i'd\" : \"I would\",\n",
    "                \"i'd\" : \"I had\",\n",
    "                \"i'll\" : \"I will\",\n",
    "                \"i'm\" : \"I am\",\n",
    "                \"isn't\" : \"is not\",\n",
    "                \"it's\" : \"it is\",\n",
    "                \"it'll\":\"it will\",\n",
    "                \"i've\" : \"I have\",\n",
    "                \"let's\" : \"let us\",\n",
    "                \"mightn't\" : \"might not\",\n",
    "                \"mustn't\" : \"must not\",\n",
    "                \"shan't\" : \"shall not\",\n",
    "                \"she'd\" : \"she would\",\n",
    "                \"she'll\" : \"she will\",\n",
    "                \"she's\" : \"she is\",\n",
    "                \"shouldn't\" : \"should not\",\n",
    "                \"that's\" : \"that is\",\n",
    "                \"there's\" : \"there is\",\n",
    "                \"they'd\" : \"they would\",\n",
    "                \"they'll\" : \"they will\",\n",
    "                \"they're\" : \"they are\",\n",
    "                \"they've\" : \"they have\",\n",
    "                \"we'd\" : \"we would\",\n",
    "                \"we're\" : \"we are\",\n",
    "                \"weren't\" : \"were not\",\n",
    "                \"we've\" : \"we have\",\n",
    "                \"what'll\" : \"what will\",\n",
    "                \"what're\" : \"what are\",\n",
    "                \"what's\" : \"what is\",\n",
    "                \"what've\" : \"what have\",\n",
    "                \"where's\" : \"where is\",\n",
    "                \"who'd\" : \"who would\",\n",
    "                \"who'll\" : \"who will\",\n",
    "                \"who're\" : \"who are\",\n",
    "                \"who's\" : \"who is\",\n",
    "                \"who've\" : \"who have\",\n",
    "                \"won't\" : \"will not\",\n",
    "                \"wouldn't\" : \"would not\",\n",
    "                \"you'd\" : \"you would\",\n",
    "                \"you'll\" : \"you will\",\n",
    "                \"you're\" : \"you are\",\n",
    "                \"you've\" : \"you have\",\n",
    "                \"'re\": \" are\",\n",
    "                \"wasn't\": \"was not\",\n",
    "                \"we'll\":\" will\",\n",
    "                \"didn't\": \"did not\",\n",
    "                \"tryin'\":\"trying\"\n",
    "}\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "def get_polarity(text):\n",
    "    try:\n",
    "        textblob = TextBlob(unicode(text, 'utf-8'))\n",
    "        pol = textblob.sentiment.polarity\n",
    "    except:\n",
    "        pol = 0.0\n",
    "    return pol\n",
    "\n",
    "def get_subjectivity(text):\n",
    "    try:\n",
    "        textblob = TextBlob(unicode(text, 'utf-8'))\n",
    "        subj = textblob.sentiment.subjectivity\n",
    "    except:\n",
    "        subj = 0.0\n",
    "    return subj\n",
    "\n",
    "def get_parts_of_speech_count(x):\n",
    "    cnt = {\n",
    "        'noun': 0,\n",
    "        'pron': 0,\n",
    "        'verb': 0,\n",
    "        'adj': 0,\n",
    "        'adv': 0\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        wiki = TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            for pos_tag_type in cnt.keys():\n",
    "                if ppo in pos_dic[pos_tag_type]:\n",
    "                    cnt[pos_tag_type] += 1\n",
    "    except:\n",
    "        pass\n",
    "    return [cnt[i] for i in cnt.keys()]\n",
    "\n",
    "def sent2vec(description, embedding_index_1, embedding_index_2 = None):\n",
    "    M = []\n",
    "    for w in word_tokenize(description):\n",
    "        if not w.isalpha():\n",
    "            continue\n",
    "        if w in embedding_index_1 and w in embedding_index_2:\n",
    "            embedding_vector = np.mean([embedding_index_1[w], embedding_index_2[w]], axis = 0)\n",
    "            M.append(embedding_vector)\n",
    "            continue\n",
    "        if w in embedding_index_1:\n",
    "            embedding_vector = embedding_index_1[w]\n",
    "            M.append(embedding_vector)\n",
    "            continue\n",
    "        if w in embedding_index_2:\n",
    "            embedding_vector = embedding_index_2[w]\n",
    "            M.append(embedding_vector)\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis = 0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "def word2vec_feature(df, groupby, target, size):\n",
    "    df_bag = pd.DataFrame(df[[groupby, target]])\n",
    "    df_bag[target] = df_bag[target].astype(str)\n",
    "    df_bag[target].fillna('NAN', inplace = True)\n",
    "    df_bag = df_bag.groupby(groupby, as_index = False)[target].agg({'list':(lambda x: list(x))}).reset_index()\n",
    "    doc_list = list(df_bag['list'].values)\n",
    "    w2v = Word2Vec(doc_list, size = size, window = 3, min_count = 1)\n",
    "    vocab_keys = list(w2v.wv.vocab.keys())\n",
    "    w2v_array = []\n",
    "    for v in vocab_keys:\n",
    "        w2v_array.append(list(w2v.wv[v]))\n",
    "    df_w2v = pd.DataFrame()\n",
    "    df_w2v['vocab_keys'] = vocab_keys    \n",
    "    df_w2v = pd.concat([df_w2v, pd.DataFrame(w2v_array)], axis = 1)\n",
    "    df_w2v.columns = [target] + ['w2v_%s_%s_%d'%(groupby, target, x) for x in range(size)]\n",
    "    print ('df_w2v:' + str(df_w2v.shape))\n",
    "    return df_w2v\n",
    "\n",
    "def get_textual_features_from_description(df):\n",
    "    \n",
    "    # Fill NA\n",
    "    df[[\"Description\"]] = df[[\"Description\"]].fillna(\"none\")\n",
    "    \n",
    "    # Get stopwords count\n",
    "    print(\"Get stopword count\")\n",
    "    df['Description_stopword_count'] = df['Description'].apply(lambda x: len([wrd for wrd in x.split() if wrd.lower() in stop_words]))\n",
    "    \n",
    "    # Convert to lower\n",
    "    print(\"Convert to lower\")\n",
    "    df[\"Description\"] = df[\"Description\"].apply(lambda x: x.lower())\n",
    "    \n",
    "    # Clean the text\n",
    "    print(\"Clean the text\")\n",
    "    df[\"Description\"] = df[\"Description\"].apply(lambda x: clean_text(x))\n",
    "    \n",
    "    # Clean spellings\n",
    "    print(\"Convert the spellings\")\n",
    "    df[\"Description\"] = df[\"Description\"].apply(lambda x: replace_typical_misspell(x))\n",
    "    \n",
    "    # Remove stopwords\n",
    "    df[\"Description\"] = df[\"Description\"].apply(lambda x: \" \".join([item for item in x if item not in stop_words]))\n",
    "    \n",
    "    # Get character count\n",
    "    print(\"Get character count\")\n",
    "    df['Description_character_count'] = df['Description'].str.len()\n",
    "    \n",
    "    # Get word count\n",
    "    print(\"Get subjectivity\")\n",
    "    df['Description_word_count'] = df['Description'].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    # Get word density\n",
    "    print(\"Get word density\")\n",
    "    df['Description_word_density'] = df['Description_character_count']/(df['Description_word_count'] + 1)\n",
    "    \n",
    "    # Get number of punctuations in a description\n",
    "    print(\"Get punctuation count\")\n",
    "    df['Description_punctuation_count'] = df['Description'].apply(lambda x: len(\"\".join(_ for _ in x if _ in punctuation)))\n",
    "    \n",
    "    # Get polarity\n",
    "    print(\"Get polarity\")\n",
    "    df['Description_polarity'] = df['Description'].apply(get_polarity)\n",
    "    \n",
    "    # Get subjectivity\n",
    "    print(\"Get subjectivity\")\n",
    "    df['Description_subjectivity'] = df['Description'].apply(get_subjectivity)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "4fd34b4b3ff0f60005fe960619dc00193fb09ceb"
   },
   "outputs": [],
   "source": [
    "def description(petid, dataclass = \"train\"):\n",
    "    try:\n",
    "        with open(\"../input/petfinder-adoption-prediction/\" + f\"{dataclass}_metadata/{petid}-1.json\", 'r') as f:\n",
    "            im_meta = json.load(f)\n",
    "        descriptions = \"\"\n",
    "        for desc in im_meta['labelAnnotations']:\n",
    "            descriptions += desc['description'] + \" \"\n",
    "        return descriptions\n",
    "    except (FileNotFoundError, KeyError):\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Another error: {e}\")\n",
    "        return None\n",
    "\n",
    "dog_str = re.compile(r\"((.*\\W)|(^))dog((\\W.*)|($))\", re.IGNORECASE)\n",
    "cat_str = re.compile(r\"((.*\\W)|(^))cat((\\W.*)|($))\", re.IGNORECASE)\n",
    "def dog_label(description):\n",
    "    \"\"\"dog label from Google vision:\n",
    "    0 = no label\n",
    "    1 = 'dog' was in the tag\n",
    "    \"\"\"\n",
    "    return [0, 1][dog_str.search(description) is not None]\n",
    "    \n",
    "def cat_label(description):\n",
    "    \"\"\"cat label from Google vision:\n",
    "    0 = no label\n",
    "    1 = 'cat' was in the tag\n",
    "    \"\"\"\n",
    "    return [0, 1][cat_str.search(description) is not None]\n",
    "\n",
    "description_fies = [dog_label, cat_label]\n",
    "def get_dog_cat_label_features(df, dataclass = \"train\"):\n",
    "    ftr = np.zeros((len(df), len(description_fies)), dtype = np.int)\n",
    "    for i, petid in enumerate(df.PetID):\n",
    "        desc = description(petid, dataclass)\n",
    "        if desc is not None:\n",
    "            for j, f in enumerate(description_fies):\n",
    "                ftr[i,j] = f(desc)\n",
    "    for i, f in enumerate(description_fies):\n",
    "        df[f.__name__] = ftr[:,i]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "b999199c2bcb595f2e23e6abd225dca8e7edef29"
   },
   "outputs": [],
   "source": [
    "# Reference - https://www.kaggle.com/christofhenkel/extract-image-features-from-pretrained-nn/notebook\n",
    "img_size = 256\n",
    "batch_size = 16\n",
    "\n",
    "def resize_to_square(im):\n",
    "    old_size = im.shape[:2] # old_size is in (height, width) format\n",
    "    ratio = float(img_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    \n",
    "    # new_size should be in (width, height) format\n",
    "    im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "    delta_w = img_size - new_size[1]\n",
    "    delta_h = img_size - new_size[0]\n",
    "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "    color = [0, 0, 0]\n",
    "    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n",
    "    return new_im\n",
    "\n",
    "def load_image(path, pet_id):\n",
    "    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n",
    "    new_image = resize_to_square(image)\n",
    "    new_image = preprocess_input(new_image)\n",
    "    return new_image\n",
    "\n",
    "def get_raw_image_features(df, df_type = \"train\"):\n",
    "    pet_ids = df['PetID'].values\n",
    "    n_batches = len(pet_ids) // batch_size + 1\n",
    "    \n",
    "    inp = Input((256,256,3))\n",
    "    backbone = DenseNet121(input_tensor = inp, include_top = False, weights = '../input/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "    x = backbone.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\n",
    "    x = AveragePooling1D(4)(x)\n",
    "    out = Lambda(lambda x: x[:,:,0])(x)\n",
    "\n",
    "    m = Model(inp, out)\n",
    "    \n",
    "    features = {}\n",
    "    for b in tqdm_notebook(range(n_batches)):\n",
    "        start = b*batch_size\n",
    "        end = (b+1)*batch_size\n",
    "        batch_pets = pet_ids[start:end]\n",
    "        batch_images = np.zeros((len(batch_pets), img_size, img_size, 3))\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            try:\n",
    "                batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/\" + df_type + \"_images/\", pet_id)\n",
    "            except:\n",
    "                pass\n",
    "        batch_preds = m.predict(batch_images)\n",
    "        for i, pet_id in enumerate(batch_pets):\n",
    "            features[pet_id] = batch_preds[i]\n",
    "            \n",
    "    feats_df = pd.DataFrame.from_dict(features, orient = 'index').reset_index()\n",
    "    feats_df['PetID'] = feats_df['index']\n",
    "    feats_df = feats_df.drop('index', 1)\n",
    "    df = pd.merge(df, feats_df, how = 'left', on = ['PetID'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "c30180b140ab8cacbf4099c6807382ffe81de9e8"
   },
   "outputs": [],
   "source": [
    "def get_aggregate_features(df):\n",
    "    \n",
    "#     # Create dummies\n",
    "#     df = pd.get_dummies(df, columns = ['Vaccinated', 'Dewormed', 'Sterilized', 'Health'])\n",
    "    \n",
    "    # Some aggregate features\n",
    "    aggs = {}\n",
    "    aggs['Fee'] = ['mean', 'std', 'min', 'max']\n",
    "    aggs['Quantity'] = ['mean', 'std', 'min', 'max']\n",
    "    aggs['Age'] = ['mean', 'std', 'min', 'max']\n",
    "    aggs['PhotoAmt'] = ['mean', 'std', 'min', 'max']\n",
    "    aggs['VideoAmt'] = ['mean', 'std', 'min', 'max']\n",
    "    aggs['FurLength'] = ['mean', 'std', 'min', 'max']\n",
    "    aggs['MaturitySize'] = ['mean', 'std', 'min', 'max']\n",
    "    aggs['Vaccinated'] = ['sum', 'mean']\n",
    "    aggs['Dewormed'] = ['sum', 'mean']\n",
    "    aggs['Sterilized'] = ['sum', 'mean']\n",
    "    aggs['Health'] = ['sum', 'mean']\n",
    "    aggs['doc_sent_mag'] = ['mean']\n",
    "    aggs['doc_sent_score'] = ['mean']\n",
    "    aggs['vertex_x'] = ['mean']\n",
    "    aggs['vertex_y'] = ['mean']\n",
    "    aggs['bounding_confidence'] = ['mean']\n",
    "    aggs['bounding_importance'] = ['mean']\n",
    "    aggs['dominant_blue'] = ['mean']\n",
    "    aggs['dominant_green'] = ['mean']\n",
    "    aggs['dominant_red'] = ['mean']\n",
    "    aggs['dominant_pixel_frac'] = ['mean']\n",
    "    aggs['dominant_score'] = ['mean']\n",
    "    aggs['label_score'] = ['mean']\n",
    "    aggs['Description_stopword_count'] = ['mean', 'sum']\n",
    "    aggs['Description_character_count'] = ['mean', 'sum']\n",
    "    aggs['Description_word_count'] = ['mean', 'sum']\n",
    "    aggs['Description_word_density'] = ['mean']\n",
    "    aggs['Description_punctuation_count'] = ['mean', 'sum']\n",
    "    aggs['Description_polarity'] = ['mean']\n",
    "    aggs['Description_subjectivity'] = ['mean']\n",
    "#     aggs['Gender'] = ['mean', 'std', 'min', 'max']\n",
    "#     aggs['Color1'] = ['mean', 'std', 'min', 'max']\n",
    "#     aggs['Color2'] = ['mean', 'std', 'min', 'max']\n",
    "#     aggs['Color3'] = ['mean', 'std', 'min', 'max']\n",
    "#     aggs['Vaccinated_1'] = ['mean',  'std','min', 'max']\n",
    "#     aggs['Vaccinated_2'] = ['mean',  'std','min', 'max']\n",
    "#     aggs['Vaccinated_3'] = ['mean',  'std','min', 'max']\n",
    "#     aggs['Dewormed_1'] = ['mean',  'std','min', 'max']\n",
    "#     aggs['Dewormed_2'] = ['mean',  'std','min', 'max']\n",
    "#     aggs['Dewormed_3'] = ['mean',  'std','min', 'max']\n",
    "#     aggs['Sterilized_1'] = ['mean',  'std','min', 'max']\n",
    "#     aggs['Sterilized_2'] = ['mean',  'std','min', 'max']\n",
    "#     aggs['Sterilized_3'] = ['mean',  'std','min', 'max']\n",
    "#     aggs['Health_1'] = ['mean',  'std','min', 'max']\n",
    "#     aggs['Health_2'] = ['mean',  'std','min', 'max']\n",
    "#     aggs['Health_3'] = ['mean',  'std','min', 'max']\n",
    "    \n",
    "    for groupby_col in ['RescuerID', 'State', 'Breed1', 'Gender']:\n",
    "        df_ = df.reset_index().groupby(groupby_col).agg(aggs)\n",
    "        df_.columns = [groupby_col + '_' + '_'.join(col).strip() for col in df_.columns.values]\n",
    "        df_.reset_index(inplace = True)\n",
    "        df = pd.merge(df, df_, on = [groupby_col], how = 'left')\n",
    "        \n",
    "    df_ = df.reset_index().groupby(['RescuerID', 'State']).agg(aggs)\n",
    "    df_.columns = ['Rescuer_State' + '_' + '_'.join(col).strip() for col in df_.columns.values]\n",
    "    df_.reset_index(inplace = True)\n",
    "    df = pd.merge(df, df_, on = ['RescuerID', 'State'], how = 'left')\n",
    "    \n",
    "    df_ = df.reset_index().groupby(['RescuerID', 'Gender']).agg(aggs)\n",
    "    df_.columns = ['Rescuer_Gender' + '_' + '_'.join(col).strip() for col in df_.columns.values]\n",
    "    df_.reset_index(inplace = True)\n",
    "    df = pd.merge(df, df_, on = ['RescuerID', 'Gender'], how = 'left')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "ed7708f1ca96b271c2cb64b00411adfa8c71294e"
   },
   "outputs": [],
   "source": [
    "class image_annotation_features():\n",
    "    \"\"\"Frame work for image annotation processing.\n",
    "    \n",
    "    Objective:  We want to go over JSON files only once and get and\n",
    "                process all that we want to get from it. Such to limit\n",
    "                disc-IO and thus time.\n",
    "    Limitation: You have to choose only profile images, or all images \n",
    "                till you find what you're lokoing for.\n",
    "               \n",
    "    Use:\n",
    "        ia = image_annotation_features()\n",
    "        ia.annotation_fies = [my_fie, my_2nd_fie]\n",
    "            my_fie should be a function taking a JSON structure and produce\n",
    "            a feature. If the function has not found what it needed and wants\n",
    "            to see the next profile picture JSON file, it should return None.\n",
    "        ia.dataclass = \"train\"\n",
    "        ia.get_ftr(train)\n",
    "        ia.dataclass = \"test\"\n",
    "        ia.get_ftr(test)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.annotation_fies = []\n",
    "        \"\"\"List of functions taking a JSON image annotation and \n",
    "           producing a single feature\"\"\"\n",
    "        self.dataclass = [\"train\",\"test\"][0]\n",
    "        \"\"\"The type of data\"\"\"\n",
    "        \n",
    "    def get_ftr(self, data):\n",
    "        ftr = [[] for i in range(len(self.annotation_fies))]\n",
    "        for petid in tqdm_notebook(data.PetID):\n",
    "            fn = '../input/petfinder-adoption-prediction/' + f\"{self.dataclass}_metadata/{petid}-1.json\"\n",
    "            self.ftrs = [None] * len(self.annotation_fies)\n",
    "            self._get_features(fn)\n",
    "            for i, f in enumerate(self.ftrs):\n",
    "                ftr[i].append(f if f is not None else 0)\n",
    "\n",
    "        for i, f in enumerate(self.annotation_fies):\n",
    "            data[f.__name__] = ftr[i]\n",
    "            \n",
    "    def _get_features(self, fn):\n",
    "        try:\n",
    "            with open(fn, 'r') as f:\n",
    "                im_meta = json.load(f)\n",
    "        except:\n",
    "            return\n",
    "        for j, f in enumerate(self.annotation_fies):\n",
    "            if self.ftrs[j] is not None:\n",
    "                continue\n",
    "            try:\n",
    "                self.ftrs[j] = f(im_meta)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "        return\n",
    "\n",
    "    def get_ftr_rcsv(self, data):\n",
    "        ftr = [[] for i in range(len(self.annotation_fies))]\n",
    "        for petid in tqdm_notebook(data.PetID):\n",
    "            fn = '../input/petfinder-adoption-prediction/' + f\"{self.dataclass}_metadata/{petid}-1.json\"\n",
    "            self.ftrs = [None] * len(self.annotation_fies)\n",
    "            self._get_features(fn)\n",
    "            if None in self.ftrs:\n",
    "                for fn_r in glob('../input/petfinder-adoption-prediction/' + f\"{self.dataclass}_metadata/{petid}-*.json\"):\n",
    "                    if fn_r == fn:\n",
    "                        continue\n",
    "                    self._get_features(fn_r)\n",
    "                    if not (None in self.ftrs):\n",
    "                        break\n",
    "                        \n",
    "            for i, f in enumerate(self.ftrs):\n",
    "                ftr[i].append(f if f is not None else 0)\n",
    "                \n",
    "        for i, f in enumerate(self.annotation_fies):\n",
    "            data[f.__name__] = ftr[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "d6a16073141c3a2f575abbf087de55b1dd5a783b"
   },
   "outputs": [],
   "source": [
    "def gv_rel_text_area_feature(im_meta):\n",
    "    text_area = 0\n",
    "    try:\n",
    "        for textbox in im_meta['textAnnotations']:\n",
    "            try:\n",
    "                rect = np.array([[c['x'], c['y']] for c in textbox['boundingPoly']['vertices']])\n",
    "                bl = np.min(rect, axis=0)\n",
    "                size = np.max(rect, axis=0) - bl\n",
    "                text_area += np.prod(size)\n",
    "            except KeyError:\n",
    "                pass\n",
    "    except KeyError:\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        imsize = im_meta['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]\n",
    "        im_area = np.prod([int(imsize['y']+1), int(imsize['x']+1)])\n",
    "    except (KeyError, TypeError):\n",
    "        return 0\n",
    "    return text_area/im_area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "f731d830f1b9bd9636c5bfae78f460fd81ad052a"
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def get_rescuer_experience(d_train, d_test):\n",
    "    \"\"\" Creates rescuer experience feature vector.\n",
    "    Vector of crossproduct of Type and maturity size (8 elements).\n",
    "    Each element has the experience in that category of the rescuer,\n",
    "    normalized to the total experience in the total (train+test) dataset.\n",
    "    Returns:\n",
    "        Nothing, columns added to the Dataframes\n",
    "    \"\"\"\n",
    "    \n",
    "    d = d_train.append(d_test, sort=True)\n",
    "    feature = np.zeros((len(d), 8), np.float32)\n",
    "    types = d.Type.map(str)\n",
    "    maturities = d.MaturitySize.map(str)\n",
    "    h = Counter(d.RescuerID.values + \"_\" + types + \"_\" + maturities)\n",
    "    keys = [f\"_{i}_{j}\" for i,j in product(set(types), set(maturities))]\n",
    "    for i, rid in enumerate(d['RescuerID']):\n",
    "        feature[i] = [h[rid+k] for k in keys]\n",
    "    for i, k in enumerate(keys):\n",
    "        d_train[\"rescuer\" + k] = feature[0:len(d_train),i]\n",
    "        d_test[\"rescuer\" + k] = feature[len(d_train):,i]\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "7b5bd695e9eb94b53d491f1839210c82881a26c1"
   },
   "outputs": [],
   "source": [
    "class GVBreedDesc():\n",
    "    def __init__(self):\n",
    "        # load breeds\n",
    "        breeds = pd.read_csv('../input/petfinder-adoption-prediction/' + \"breed_labels.csv\")\n",
    "        # compile regex\n",
    "        self.breed_regexs = []\n",
    "        for ind, breedname in zip(breeds.BreedID, breeds.BreedName):\n",
    "            self.breed_regexs.append((ind, re.compile(f\".*{breedname}.*\", re.IGNORECASE)))\n",
    "        \n",
    "    def _description(self, im_meta):\n",
    "        try:\n",
    "            descriptions = \"\"\n",
    "            for desc in im_meta['labelAnnotations']:\n",
    "                descriptions += desc['description'] + \" \"\n",
    "        except KeyError:\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            print(f\"Another error: {e}\")\n",
    "            pass\n",
    "        return descriptions\n",
    "\n",
    "    def gv_breed_feature(self, im_meta):\n",
    "        desc = self._description(im_meta)\n",
    "        for i, regex in self.breed_regexs:\n",
    "            if regex.match(desc):\n",
    "                return i\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "4fa81e7582ecdf3f04186f07d98abc6298e361d3"
   },
   "outputs": [],
   "source": [
    "def get_svd_features(train, test):\n",
    "    tfv = TfidfVectorizer(min_df=3,  max_features=10000,\n",
    "        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "        stop_words = 'english')\n",
    "\n",
    "    train_desc = train[\"Description\"].fillna(\"none\").values\n",
    "    test_desc = test[\"Description\"].fillna(\"none\").values\n",
    "\n",
    "    tfv.fit(list(train_desc) + list(test_desc))\n",
    "    X_train =  tfv.transform(train_desc)\n",
    "    X_test = tfv.transform(test_desc)\n",
    "\n",
    "    svd = TruncatedSVD(n_components = 120, random_state = 420)\n",
    "    svd.fit(X_train)\n",
    "    X_train = svd.transform(X_train)\n",
    "    X_test = svd.transform(X_test)\n",
    "\n",
    "    X_train = pd.DataFrame(X_train, columns=['svd_{}'.format(i) for i in range(120)])\n",
    "    X_test = pd.DataFrame(X_test, columns=['svd_{}'.format(i) for i in range(120)])\n",
    "    train = pd.concat((train, X_train), axis = 1)\n",
    "    test = pd.concat((test, X_test), axis = 1)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "f9230db38c6457cc924092f3b70dc92cf1446cbc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get stopword count\n",
      "Convert to lower\n",
      "Clean the text\n",
      "Convert the spellings\n",
      "Get character count\n",
      "Get subjectivity\n",
      "Get word density\n",
      "Get punctuation count\n",
      "Get polarity\n",
      "Get subjectivity\n",
      "Get stopword count\n",
      "Convert to lower\n",
      "Clean the text\n",
      "Convert the spellings\n",
      "Get character count\n",
      "Get subjectivity\n",
      "Get word density\n",
      "Get punctuation count\n",
      "Get polarity\n",
      "Get subjectivity\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d45aa057afc4296bfd61d99618ccb87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=938), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e6635526a04503839227e8ac856f2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=249), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cols_to_remove = ['Name', 'RescuerID', 'Description', 'PetID']\n",
    "\n",
    "train, test = train_orig.copy(), test_orig.copy()\n",
    "\n",
    "# # Get text area in image feature\n",
    "# ia = image_annotation_features()\n",
    "# ia.annotation_fies = [gv_rel_text_area_feature]\n",
    "# ia.dataclass = \"test\"\n",
    "# ia.get_ftr(test)\n",
    "# ia.dataclass = \"train\"\n",
    "# feats = ia.get_ftr(train)\n",
    "\n",
    "# # # Get GV Breed features\n",
    "# gv_breed_desc = GVBreedDesc()\n",
    "# ia = image_annotation_features()\n",
    "# ia.annotation_fies = [gv_breed_desc.gv_breed_feature]\n",
    "# ia.dataclass = \"test\"\n",
    "# ia.get_ftr_rcsv(test)\n",
    "# ia.dataclass = \"train\"\n",
    "# ia.get_ftr_rcsv(train)\n",
    "\n",
    "# Get rescuer experience feature\n",
    "get_rescuer_experience(train, test)\n",
    "\n",
    "# Get textual features\n",
    "train = get_textual_features_from_description(train)\n",
    "test = get_textual_features_from_description(test)\n",
    "\n",
    "# Get TFIDF and SVD features\n",
    "train, test = get_svd_features(train, test)\n",
    "\n",
    "# Get description sentiment from JSON data\n",
    "train = get_description_sentiment(train, \"train\")\n",
    "test = get_description_sentiment(test, \"test\")\n",
    "\n",
    "# Get image metadata\n",
    "train = get_image_metadata(train, \"train\")\n",
    "test = get_image_metadata(test, \"test\")\n",
    "\n",
    "# Get label-annotation features\n",
    "# train = get_dog_cat_label_features(train, \"train\")\n",
    "# test = get_dog_cat_label_features(test, \"test\")\n",
    "\n",
    "# Get raw image features\n",
    "train = get_raw_image_features(train, \"train\")\n",
    "test = get_raw_image_features(test, \"test\")\n",
    "\n",
    "# Create some aggregate features\n",
    "train = get_aggregate_features(train)\n",
    "test = get_aggregate_features(test)\n",
    "\n",
    "# Save RescuerID for fold grouping\n",
    "rescuerid = train.RescuerID\n",
    "\n",
    "target = train['AdoptionSpeed']\n",
    "train = train.drop('AdoptionSpeed', 1)\n",
    "\n",
    "# Remove unnecessary columns\n",
    "train.drop(cols_to_remove, 1, inplace = True)\n",
    "test.drop(cols_to_remove, 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "e23a34fcccc5f3b582cbe138b17be6af50b33c50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((14993, 776), (3972, 776))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "360fe0003e3b059bfdf1d760975aa3b55482d514",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Age</th>\n",
       "      <th>Breed1</th>\n",
       "      <th>Breed2</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>Color3</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>Vaccinated</th>\n",
       "      <th>Dewormed</th>\n",
       "      <th>Sterilized</th>\n",
       "      <th>Health</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Fee</th>\n",
       "      <th>State</th>\n",
       "      <th>VideoAmt</th>\n",
       "      <th>PhotoAmt</th>\n",
       "      <th>rescuer_2_4</th>\n",
       "      <th>rescuer_2_3</th>\n",
       "      <th>rescuer_2_2</th>\n",
       "      <th>rescuer_2_1</th>\n",
       "      <th>rescuer_1_4</th>\n",
       "      <th>rescuer_1_3</th>\n",
       "      <th>...</th>\n",
       "      <th>Rescuer_Gender_Health_sum</th>\n",
       "      <th>Rescuer_Gender_Health_mean</th>\n",
       "      <th>Rescuer_Gender_doc_sent_mag_mean</th>\n",
       "      <th>Rescuer_Gender_doc_sent_score_mean</th>\n",
       "      <th>Rescuer_Gender_vertex_x_mean</th>\n",
       "      <th>Rescuer_Gender_vertex_y_mean</th>\n",
       "      <th>Rescuer_Gender_bounding_confidence_mean</th>\n",
       "      <th>Rescuer_Gender_bounding_importance_mean</th>\n",
       "      <th>Rescuer_Gender_dominant_blue_mean</th>\n",
       "      <th>Rescuer_Gender_dominant_green_mean</th>\n",
       "      <th>Rescuer_Gender_dominant_red_mean</th>\n",
       "      <th>Rescuer_Gender_dominant_pixel_frac_mean</th>\n",
       "      <th>Rescuer_Gender_dominant_score_mean</th>\n",
       "      <th>Rescuer_Gender_label_score_mean</th>\n",
       "      <th>Rescuer_Gender_Description_stopword_count_mean</th>\n",
       "      <th>Rescuer_Gender_Description_stopword_count_sum</th>\n",
       "      <th>Rescuer_Gender_Description_character_count_mean</th>\n",
       "      <th>Rescuer_Gender_Description_character_count_sum</th>\n",
       "      <th>Rescuer_Gender_Description_word_count_mean</th>\n",
       "      <th>Rescuer_Gender_Description_word_count_sum</th>\n",
       "      <th>Rescuer_Gender_Description_word_density_mean</th>\n",
       "      <th>Rescuer_Gender_Description_punctuation_count_mean</th>\n",
       "      <th>Rescuer_Gender_Description_punctuation_count_sum</th>\n",
       "      <th>Rescuer_Gender_Description_polarity_mean</th>\n",
       "      <th>Rescuer_Gender_Description_subjectivity_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>41326</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>359.000000</td>\n",
       "      <td>479.000000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.393910</td>\n",
       "      <td>0.302789</td>\n",
       "      <td>0.990786</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>34</td>\n",
       "      <td>501.000000</td>\n",
       "      <td>501</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>167</td>\n",
       "      <td>2.982143</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41401</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>348.500000</td>\n",
       "      <td>387.500000</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.500000</td>\n",
       "      <td>23.500000</td>\n",
       "      <td>24.500000</td>\n",
       "      <td>0.185342</td>\n",
       "      <td>0.353332</td>\n",
       "      <td>0.983590</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>12</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>137</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>43</td>\n",
       "      <td>3.113636</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41326</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>...</td>\n",
       "      <td>148</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.473649</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>369.372174</td>\n",
       "      <td>443.066779</td>\n",
       "      <td>0.802522</td>\n",
       "      <td>0.993987</td>\n",
       "      <td>91.396673</td>\n",
       "      <td>104.469023</td>\n",
       "      <td>126.513345</td>\n",
       "      <td>0.099522</td>\n",
       "      <td>0.250549</td>\n",
       "      <td>0.967341</td>\n",
       "      <td>8.885135</td>\n",
       "      <td>1315</td>\n",
       "      <td>181.554054</td>\n",
       "      <td>26870</td>\n",
       "      <td>62.648649</td>\n",
       "      <td>9272</td>\n",
       "      <td>2.824710</td>\n",
       "      <td>2.358108</td>\n",
       "      <td>349</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>41401</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>1.035714</td>\n",
       "      <td>1.117857</td>\n",
       "      <td>0.357143</td>\n",
       "      <td>371.645542</td>\n",
       "      <td>380.128394</td>\n",
       "      <td>0.738244</td>\n",
       "      <td>0.927375</td>\n",
       "      <td>87.666424</td>\n",
       "      <td>102.261672</td>\n",
       "      <td>118.285150</td>\n",
       "      <td>0.070447</td>\n",
       "      <td>0.230839</td>\n",
       "      <td>0.881987</td>\n",
       "      <td>34.214286</td>\n",
       "      <td>958</td>\n",
       "      <td>574.714286</td>\n",
       "      <td>16092</td>\n",
       "      <td>189.750000</td>\n",
       "      <td>5313</td>\n",
       "      <td>3.008534</td>\n",
       "      <td>10.821429</td>\n",
       "      <td>303</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41326</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>...</td>\n",
       "      <td>66</td>\n",
       "      <td>1.031250</td>\n",
       "      <td>1.943750</td>\n",
       "      <td>0.290625</td>\n",
       "      <td>383.083333</td>\n",
       "      <td>452.079688</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.988359</td>\n",
       "      <td>100.985677</td>\n",
       "      <td>119.224219</td>\n",
       "      <td>136.935417</td>\n",
       "      <td>0.118809</td>\n",
       "      <td>0.264074</td>\n",
       "      <td>0.964085</td>\n",
       "      <td>22.515625</td>\n",
       "      <td>1441</td>\n",
       "      <td>333.593750</td>\n",
       "      <td>21350</td>\n",
       "      <td>107.109375</td>\n",
       "      <td>6855</td>\n",
       "      <td>3.090985</td>\n",
       "      <td>6.265625</td>\n",
       "      <td>401</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 776 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type  Age  Breed1  Breed2  Gender  Color1  Color2  Color3  MaturitySize  \\\n",
       "0     2    3     299       0       1       1       7       0             1   \n",
       "1     2    1     265       0       1       1       2       0             2   \n",
       "2     1    1     307       0       1       2       7       0             2   \n",
       "3     1    4     307       0       2       1       2       0             2   \n",
       "4     1    1     307       0       1       1       0       0             2   \n",
       "\n",
       "   FurLength  Vaccinated  Dewormed  Sterilized  Health  Quantity  Fee  State  \\\n",
       "0          1           2         2           2       1         1  100  41326   \n",
       "1          2           3         3           3       1         1    0  41401   \n",
       "2          2           1         1           2       1         1    0  41326   \n",
       "3          1           1         1           2       1         1  150  41401   \n",
       "4          1           2         2           2       1         1    0  41326   \n",
       "\n",
       "   VideoAmt  PhotoAmt  rescuer_2_4  rescuer_2_3  rescuer_2_2  rescuer_2_1  \\\n",
       "0         0       1.0          0.0          0.0          6.0          2.0   \n",
       "1         0       2.0          0.0          0.0          1.0          0.0   \n",
       "2         0       7.0          0.0          0.0          4.0          0.0   \n",
       "3         0       8.0          0.0          0.0          8.0          1.0   \n",
       "4         0       3.0          0.0          0.0          3.0          1.0   \n",
       "\n",
       "   rescuer_1_4  rescuer_1_3                      ...                       \\\n",
       "0          0.0          0.0                      ...                        \n",
       "1          0.0          0.0                      ...                        \n",
       "2          6.0         44.0                      ...                        \n",
       "3          0.0          1.0                      ...                        \n",
       "4          0.0          4.0                      ...                        \n",
       "\n",
       "   Rescuer_Gender_Health_sum  Rescuer_Gender_Health_mean  \\\n",
       "0                          1                    1.000000   \n",
       "1                          1                    1.000000   \n",
       "2                        148                    1.000000   \n",
       "3                         29                    1.035714   \n",
       "4                         66                    1.031250   \n",
       "\n",
       "   Rescuer_Gender_doc_sent_mag_mean  Rescuer_Gender_doc_sent_score_mean  \\\n",
       "0                          2.400000                            0.300000   \n",
       "1                          0.700000                           -0.200000   \n",
       "2                          1.473649                            0.450000   \n",
       "3                          1.117857                            0.357143   \n",
       "4                          1.943750                            0.290625   \n",
       "\n",
       "   Rescuer_Gender_vertex_x_mean  Rescuer_Gender_vertex_y_mean  \\\n",
       "0                    359.000000                    479.000000   \n",
       "1                    348.500000                    387.500000   \n",
       "2                    369.372174                    443.066779   \n",
       "3                    371.645542                    380.128394   \n",
       "4                    383.083333                    452.079688   \n",
       "\n",
       "   Rescuer_Gender_bounding_confidence_mean  \\\n",
       "0                                 0.800000   \n",
       "1                                 0.800000   \n",
       "2                                 0.802522   \n",
       "3                                 0.738244   \n",
       "4                                 0.805833   \n",
       "\n",
       "   Rescuer_Gender_bounding_importance_mean  Rescuer_Gender_dominant_blue_mean  \\\n",
       "0                                 1.000000                          21.000000   \n",
       "1                                 1.000000                          17.500000   \n",
       "2                                 0.993987                          91.396673   \n",
       "3                                 0.927375                          87.666424   \n",
       "4                                 0.988359                         100.985677   \n",
       "\n",
       "   Rescuer_Gender_dominant_green_mean  Rescuer_Gender_dominant_red_mean  \\\n",
       "0                           20.000000                         25.000000   \n",
       "1                           23.500000                         24.500000   \n",
       "2                          104.469023                        126.513345   \n",
       "3                          102.261672                        118.285150   \n",
       "4                          119.224219                        136.935417   \n",
       "\n",
       "   Rescuer_Gender_dominant_pixel_frac_mean  \\\n",
       "0                                 0.393910   \n",
       "1                                 0.185342   \n",
       "2                                 0.099522   \n",
       "3                                 0.070447   \n",
       "4                                 0.118809   \n",
       "\n",
       "   Rescuer_Gender_dominant_score_mean  Rescuer_Gender_label_score_mean  \\\n",
       "0                            0.302789                         0.990786   \n",
       "1                            0.353332                         0.983590   \n",
       "2                            0.250549                         0.967341   \n",
       "3                            0.230839                         0.881987   \n",
       "4                            0.264074                         0.964085   \n",
       "\n",
       "   Rescuer_Gender_Description_stopword_count_mean  \\\n",
       "0                                       34.000000   \n",
       "1                                       12.000000   \n",
       "2                                        8.885135   \n",
       "3                                       34.214286   \n",
       "4                                       22.515625   \n",
       "\n",
       "   Rescuer_Gender_Description_stopword_count_sum  \\\n",
       "0                                             34   \n",
       "1                                             12   \n",
       "2                                           1315   \n",
       "3                                            958   \n",
       "4                                           1441   \n",
       "\n",
       "   Rescuer_Gender_Description_character_count_mean  \\\n",
       "0                                       501.000000   \n",
       "1                                       137.000000   \n",
       "2                                       181.554054   \n",
       "3                                       574.714286   \n",
       "4                                       333.593750   \n",
       "\n",
       "   Rescuer_Gender_Description_character_count_sum  \\\n",
       "0                                             501   \n",
       "1                                             137   \n",
       "2                                           26870   \n",
       "3                                           16092   \n",
       "4                                           21350   \n",
       "\n",
       "   Rescuer_Gender_Description_word_count_mean  \\\n",
       "0                                  167.000000   \n",
       "1                                   43.000000   \n",
       "2                                   62.648649   \n",
       "3                                  189.750000   \n",
       "4                                  107.109375   \n",
       "\n",
       "   Rescuer_Gender_Description_word_count_sum  \\\n",
       "0                                        167   \n",
       "1                                         43   \n",
       "2                                       9272   \n",
       "3                                       5313   \n",
       "4                                       6855   \n",
       "\n",
       "   Rescuer_Gender_Description_word_density_mean  \\\n",
       "0                                      2.982143   \n",
       "1                                      3.113636   \n",
       "2                                      2.824710   \n",
       "3                                      3.008534   \n",
       "4                                      3.090985   \n",
       "\n",
       "   Rescuer_Gender_Description_punctuation_count_mean  \\\n",
       "0                                           8.000000   \n",
       "1                                           2.000000   \n",
       "2                                           2.358108   \n",
       "3                                          10.821429   \n",
       "4                                           6.265625   \n",
       "\n",
       "   Rescuer_Gender_Description_punctuation_count_sum  \\\n",
       "0                                                 8   \n",
       "1                                                 2   \n",
       "2                                               349   \n",
       "3                                               303   \n",
       "4                                               401   \n",
       "\n",
       "   Rescuer_Gender_Description_polarity_mean  \\\n",
       "0                                       0.0   \n",
       "1                                       0.0   \n",
       "2                                       0.0   \n",
       "3                                       0.0   \n",
       "4                                       0.0   \n",
       "\n",
       "   Rescuer_Gender_Description_subjectivity_mean  \n",
       "0                                           0.0  \n",
       "1                                           0.0  \n",
       "2                                           0.0  \n",
       "3                                           0.0  \n",
       "4                                           0.0  \n",
       "\n",
       "[5 rows x 776 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fb7fb3ba0f106e16217ae44b059f7681021e7bbd"
   },
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "585a6d7777315b2fae71837a07484b8ef297b8e2"
   },
   "outputs": [],
   "source": [
    "# Metric used for this competition (Quadratic Weigthed Kappa aka Quadratic Cohen Kappa Score)\n",
    "def metric_function(y1, y2):\n",
    "    return cohen_kappa_score(y1, y2, weights = 'quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "2244d7b84e45aaec87afd2d8833374c5439aef1b"
   },
   "outputs": [],
   "source": [
    "def get_class_bounds(y, y_pred, N = 5, class0_fraction = -1):\n",
    "    ysort = np.sort(y)\n",
    "    predsort = np.sort(y_pred)\n",
    "    bounds = []\n",
    "    for ibound in range(N-1):\n",
    "        iy = len(ysort[ysort <= ibound])\n",
    "        # adjust the number of class 0 predictions?\n",
    "        if (ibound == 0) and (class0_fraction >= 0.0):\n",
    "            iy = int(class0_fraction * iy)\n",
    "        bounds.append(predsort[iy])\n",
    "    return bounds\n",
    "\n",
    "def assign_class(y_pred, boundaries):\n",
    "    \"\"\"\n",
    "    Given class boundaries in y_pred units, output integer class values\n",
    "    \"\"\"\n",
    "    y_classes = np.zeros(len(y_pred))\n",
    "    for iclass, bound in enumerate(boundaries):\n",
    "        y_classes[y_pred >= bound] = iclass + 1\n",
    "    return y_classes.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "a69e4415917cac18aed591cb3ca6d5d6c4cf6f3f"
   },
   "outputs": [],
   "source": [
    "# put some numerical values to bins\n",
    "def to_bins(x, borders):\n",
    "    for i in range(len(borders)):\n",
    "        if x <= borders[i]:\n",
    "            return i\n",
    "    return len(borders)\n",
    "\n",
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _loss(self, coef, X, y, idx):\n",
    "        X_p = np.array([to_bins(pred, coef) for pred in X])\n",
    "        ll = -metric_function(y, X_p)\n",
    "        return ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        coef = [1.5, 2.0, 2.5, 3.0]\n",
    "#         cl0fracs = np.array(np.arange(0.01, 1.001, 0.01))\n",
    "#         boundaries = []\n",
    "#         kappas = []\n",
    "#         for cl0frac in cl0fracs:\n",
    "#             boundary = get_class_bounds(y, X, class0_fraction = cl0frac)\n",
    "#             train_meta_ints = assign_class(X, boundary)\n",
    "#             kappa = metric_function(y, train_meta_ints)\n",
    "#             kappas.append(kappa)\n",
    "#             boundaries.append(boundary)\n",
    "#         max_kappa_index = np.array(kappas).argmax()\n",
    "#         coef = boundaries[max_kappa_index]\n",
    "\n",
    "        golden1 = 0.618\n",
    "        golden2 = 1 - golden1\n",
    "        ab_start = [(1, 2), (1.5, 2.5), (2, 3), (2.5, 3.5)]\n",
    "        for it1 in range(10):\n",
    "            for idx in range(4):\n",
    "                # golden section search\n",
    "                a, b = ab_start[idx]\n",
    "                # calc losses\n",
    "                coef[idx] = a\n",
    "                la = self._loss(coef, X, y, idx)\n",
    "                coef[idx] = b\n",
    "                lb = self._loss(coef, X, y, idx)\n",
    "                for it in range(20):\n",
    "                    # choose value\n",
    "                    if la > lb:\n",
    "                        a = b - (b - a) * golden1\n",
    "                        coef[idx] = a\n",
    "                        la = self._loss(coef, X, y, idx)\n",
    "                    else:\n",
    "                        b = b - (b - a) * golden2\n",
    "                        coef[idx] = b\n",
    "                        lb = self._loss(coef, X, y, idx)\n",
    "        self.coef_ = {'x': coef}\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.array([to_bins(pred, coef) for pred in X])\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "c63d3a18e54f1f6f4a6d43078a4b008ca28e3823"
   },
   "outputs": [],
   "source": [
    "# class OptimizedRounder(object):\n",
    "#     def __init__(self):\n",
    "#         self.coef_ = 0\n",
    "\n",
    "#     def _kappa_loss(self, coef, X, y):\n",
    "#         X_p = np.copy(X)\n",
    "#         for i, pred in enumerate(X_p):\n",
    "#             if pred < coef[0]:\n",
    "#                 X_p[i] = 0\n",
    "#             elif pred >= coef[0] and pred < coef[1]:\n",
    "#                 X_p[i] = 1\n",
    "#             elif pred >= coef[1] and pred < coef[2]:\n",
    "#                 X_p[i] = 2\n",
    "#             elif pred >= coef[2] and pred < coef[3]:\n",
    "#                 X_p[i] = 3\n",
    "#             else:\n",
    "#                 X_p[i] = 4\n",
    "\n",
    "#         ll = metric_function(y, X_p)\n",
    "#         return -ll\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         loss_partial = partial(self._kappa_loss, X = X, y = y)\n",
    "        \n",
    "#         cl0fracs = np.array(np.arange(0.01, 1.001, 0.01))\n",
    "#         boundaries = []\n",
    "#         kappas = []\n",
    "#         for cl0frac in cl0fracs:\n",
    "#             boundary = get_class_bounds(y, X, class0_fraction = cl0frac)\n",
    "#             train_meta_ints = assign_class(X, boundary)\n",
    "#             kappa = metric_function(y, train_meta_ints)\n",
    "#             kappas.append(kappa)\n",
    "#             boundaries.append(boundary)\n",
    "#         max_kappa_index = np.array(kappas).argmax()\n",
    "#         initial_coef = boundaries[max_kappa_index]\n",
    "        \n",
    "#         self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method = 'nelder-mead')\n",
    "\n",
    "#     def predict(self, X, coef):\n",
    "#         X_p = np.copy(X)\n",
    "#         for i, pred in enumerate(X_p):\n",
    "#             if pred < coef[0]:\n",
    "#                 X_p[i] = 0\n",
    "#             elif pred >= coef[0] and pred < coef[1]:\n",
    "#                 X_p[i] = 1\n",
    "#             elif pred >= coef[1] and pred < coef[2]:\n",
    "#                 X_p[i] = 2\n",
    "#             elif pred >= coef[2] and pred < coef[3]:\n",
    "#                 X_p[i] = 3\n",
    "#             else:\n",
    "#                 X_p[i] = 4\n",
    "#         return X_p\n",
    "\n",
    "#     def coefficients(self):\n",
    "#         return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "b0aa90db89b45748670560d4d94a9e69f7cdff1e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\tvalid_0's l2: 1.18075\tvalid_0's rmse: 1.08662\n",
      "[200]\tvalid_0's l2: 1.13093\tvalid_0's rmse: 1.06345\n",
      "[300]\tvalid_0's l2: 1.10496\tvalid_0's rmse: 1.05117\n",
      "[400]\tvalid_0's l2: 1.09394\tvalid_0's rmse: 1.04592\n",
      "[500]\tvalid_0's l2: 1.08684\tvalid_0's rmse: 1.04251\n",
      "[600]\tvalid_0's l2: 1.08289\tvalid_0's rmse: 1.04062\n",
      "[700]\tvalid_0's l2: 1.07977\tvalid_0's rmse: 1.03912\n",
      "[800]\tvalid_0's l2: 1.07771\tvalid_0's rmse: 1.03813\n",
      "[900]\tvalid_0's l2: 1.07642\tvalid_0's rmse: 1.03751\n",
      "[1000]\tvalid_0's l2: 1.0756\tvalid_0's rmse: 1.03711\n",
      "[1100]\tvalid_0's l2: 1.07543\tvalid_0's rmse: 1.03703\n",
      "[1200]\tvalid_0's l2: 1.07482\tvalid_0's rmse: 1.03674\n",
      "[1300]\tvalid_0's l2: 1.07373\tvalid_0's rmse: 1.03621\n",
      "[1400]\tvalid_0's l2: 1.07339\tvalid_0's rmse: 1.03605\n",
      "[1500]\tvalid_0's l2: 1.07297\tvalid_0's rmse: 1.03584\n",
      "[1600]\tvalid_0's l2: 1.07288\tvalid_0's rmse: 1.0358\n",
      "[1700]\tvalid_0's l2: 1.072\tvalid_0's rmse: 1.03537\n",
      "[1800]\tvalid_0's l2: 1.07116\tvalid_0's rmse: 1.03497\n",
      "[1900]\tvalid_0's l2: 1.07055\tvalid_0's rmse: 1.03468\n",
      "[2000]\tvalid_0's l2: 1.07019\tvalid_0's rmse: 1.0345\n",
      "[2100]\tvalid_0's l2: 1.0699\tvalid_0's rmse: 1.03436\n",
      "[2200]\tvalid_0's l2: 1.06959\tvalid_0's rmse: 1.03421\n",
      "[2300]\tvalid_0's l2: 1.06966\tvalid_0's rmse: 1.03424\n",
      "[2400]\tvalid_0's l2: 1.06974\tvalid_0's rmse: 1.03428\n",
      "[2500]\tvalid_0's l2: 1.06951\tvalid_0's rmse: 1.03417\n",
      "[2600]\tvalid_0's l2: 1.06937\tvalid_0's rmse: 1.03411\n",
      "[2700]\tvalid_0's l2: 1.06942\tvalid_0's rmse: 1.03413\n",
      "[2800]\tvalid_0's l2: 1.06968\tvalid_0's rmse: 1.03425\n",
      "[2900]\tvalid_0's l2: 1.06936\tvalid_0's rmse: 1.0341\n",
      "[3000]\tvalid_0's l2: 1.0693\tvalid_0's rmse: 1.03407\n",
      "[3100]\tvalid_0's l2: 1.06919\tvalid_0's rmse: 1.03402\n",
      "[3200]\tvalid_0's l2: 1.06919\tvalid_0's rmse: 1.03402\n",
      "[3300]\tvalid_0's l2: 1.06908\tvalid_0's rmse: 1.03396\n",
      "[3400]\tvalid_0's l2: 1.06894\tvalid_0's rmse: 1.0339\n",
      "[3500]\tvalid_0's l2: 1.06884\tvalid_0's rmse: 1.03385\n",
      "[3600]\tvalid_0's l2: 1.06889\tvalid_0's rmse: 1.03387\n",
      "[3700]\tvalid_0's l2: 1.06907\tvalid_0's rmse: 1.03396\n",
      "[3800]\tvalid_0's l2: 1.06897\tvalid_0's rmse: 1.03391\n",
      "[3900]\tvalid_0's l2: 1.06899\tvalid_0's rmse: 1.03392\n",
      "Early stopping, best iteration is:\n",
      "[3498]\tvalid_0's l2: 1.06882\tvalid_0's rmse: 1.03384\n",
      "Fold = 1. QWK = 0.42391605406902555.\n",
      "\n",
      "\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\tvalid_0's l2: 1.24903\tvalid_0's rmse: 1.1176\n",
      "[200]\tvalid_0's l2: 1.17408\tvalid_0's rmse: 1.08355\n",
      "[300]\tvalid_0's l2: 1.13662\tvalid_0's rmse: 1.06612\n",
      "[400]\tvalid_0's l2: 1.11498\tvalid_0's rmse: 1.05593\n",
      "[500]\tvalid_0's l2: 1.10339\tvalid_0's rmse: 1.05043\n",
      "[600]\tvalid_0's l2: 1.09701\tvalid_0's rmse: 1.04738\n",
      "[700]\tvalid_0's l2: 1.09192\tvalid_0's rmse: 1.04495\n",
      "[800]\tvalid_0's l2: 1.08907\tvalid_0's rmse: 1.04358\n",
      "[900]\tvalid_0's l2: 1.08712\tvalid_0's rmse: 1.04265\n",
      "[1000]\tvalid_0's l2: 1.08494\tvalid_0's rmse: 1.0416\n",
      "[1100]\tvalid_0's l2: 1.08367\tvalid_0's rmse: 1.041\n",
      "[1200]\tvalid_0's l2: 1.08255\tvalid_0's rmse: 1.04046\n",
      "[1300]\tvalid_0's l2: 1.08174\tvalid_0's rmse: 1.04007\n",
      "[1400]\tvalid_0's l2: 1.08032\tvalid_0's rmse: 1.03938\n",
      "[1500]\tvalid_0's l2: 1.07984\tvalid_0's rmse: 1.03915\n",
      "[1600]\tvalid_0's l2: 1.07974\tvalid_0's rmse: 1.03911\n",
      "[1700]\tvalid_0's l2: 1.07937\tvalid_0's rmse: 1.03893\n",
      "[1800]\tvalid_0's l2: 1.0786\tvalid_0's rmse: 1.03856\n",
      "[1900]\tvalid_0's l2: 1.07845\tvalid_0's rmse: 1.03849\n",
      "[2000]\tvalid_0's l2: 1.07799\tvalid_0's rmse: 1.03826\n",
      "[2100]\tvalid_0's l2: 1.07802\tvalid_0's rmse: 1.03828\n",
      "[2200]\tvalid_0's l2: 1.07804\tvalid_0's rmse: 1.03829\n",
      "[2300]\tvalid_0's l2: 1.0775\tvalid_0's rmse: 1.03802\n",
      "[2400]\tvalid_0's l2: 1.0779\tvalid_0's rmse: 1.03822\n",
      "[2500]\tvalid_0's l2: 1.07769\tvalid_0's rmse: 1.03812\n",
      "[2600]\tvalid_0's l2: 1.07732\tvalid_0's rmse: 1.03794\n",
      "[2700]\tvalid_0's l2: 1.07714\tvalid_0's rmse: 1.03786\n",
      "[2800]\tvalid_0's l2: 1.07703\tvalid_0's rmse: 1.0378\n",
      "[2900]\tvalid_0's l2: 1.07683\tvalid_0's rmse: 1.0377\n",
      "[3000]\tvalid_0's l2: 1.07692\tvalid_0's rmse: 1.03775\n",
      "[3100]\tvalid_0's l2: 1.07688\tvalid_0's rmse: 1.03773\n",
      "[3200]\tvalid_0's l2: 1.07678\tvalid_0's rmse: 1.03768\n",
      "[3300]\tvalid_0's l2: 1.07689\tvalid_0's rmse: 1.03773\n",
      "[3400]\tvalid_0's l2: 1.07681\tvalid_0's rmse: 1.0377\n",
      "[3500]\tvalid_0's l2: 1.07699\tvalid_0's rmse: 1.03778\n",
      "[3600]\tvalid_0's l2: 1.07677\tvalid_0's rmse: 1.03768\n",
      "[3700]\tvalid_0's l2: 1.07672\tvalid_0's rmse: 1.03765\n",
      "[3800]\tvalid_0's l2: 1.0768\tvalid_0's rmse: 1.03769\n",
      "[3900]\tvalid_0's l2: 1.07678\tvalid_0's rmse: 1.03768\n",
      "[4000]\tvalid_0's l2: 1.07668\tvalid_0's rmse: 1.03763\n",
      "[4100]\tvalid_0's l2: 1.07659\tvalid_0's rmse: 1.03759\n",
      "[4200]\tvalid_0's l2: 1.07658\tvalid_0's rmse: 1.03758\n",
      "[4300]\tvalid_0's l2: 1.0765\tvalid_0's rmse: 1.03755\n",
      "[4400]\tvalid_0's l2: 1.07649\tvalid_0's rmse: 1.03754\n",
      "[4500]\tvalid_0's l2: 1.07645\tvalid_0's rmse: 1.03752\n",
      "[4600]\tvalid_0's l2: 1.07644\tvalid_0's rmse: 1.03751\n",
      "[4700]\tvalid_0's l2: 1.07644\tvalid_0's rmse: 1.03751\n",
      "[4800]\tvalid_0's l2: 1.07654\tvalid_0's rmse: 1.03757\n",
      "[4900]\tvalid_0's l2: 1.07654\tvalid_0's rmse: 1.03756\n",
      "[5000]\tvalid_0's l2: 1.07656\tvalid_0's rmse: 1.03758\n",
      "Early stopping, best iteration is:\n",
      "[4592]\tvalid_0's l2: 1.07642\tvalid_0's rmse: 1.0375\n",
      "Fold = 2. QWK = 0.48038585774655984.\n",
      "\n",
      "\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\tvalid_0's l2: 1.19416\tvalid_0's rmse: 1.09278\n",
      "[200]\tvalid_0's l2: 1.12586\tvalid_0's rmse: 1.06107\n",
      "[300]\tvalid_0's l2: 1.09288\tvalid_0's rmse: 1.04541\n",
      "[400]\tvalid_0's l2: 1.07509\tvalid_0's rmse: 1.03687\n",
      "[500]\tvalid_0's l2: 1.06351\tvalid_0's rmse: 1.03127\n",
      "[600]\tvalid_0's l2: 1.05739\tvalid_0's rmse: 1.02829\n",
      "[700]\tvalid_0's l2: 1.05308\tvalid_0's rmse: 1.02619\n",
      "[800]\tvalid_0's l2: 1.05018\tvalid_0's rmse: 1.02478\n",
      "[900]\tvalid_0's l2: 1.04792\tvalid_0's rmse: 1.02368\n",
      "[1000]\tvalid_0's l2: 1.04707\tvalid_0's rmse: 1.02326\n",
      "[1100]\tvalid_0's l2: 1.04511\tvalid_0's rmse: 1.02231\n",
      "[1200]\tvalid_0's l2: 1.04455\tvalid_0's rmse: 1.02203\n",
      "[1300]\tvalid_0's l2: 1.04403\tvalid_0's rmse: 1.02178\n",
      "[1400]\tvalid_0's l2: 1.04385\tvalid_0's rmse: 1.02169\n",
      "[1500]\tvalid_0's l2: 1.044\tvalid_0's rmse: 1.02176\n",
      "[1600]\tvalid_0's l2: 1.04421\tvalid_0's rmse: 1.02186\n",
      "[1700]\tvalid_0's l2: 1.04465\tvalid_0's rmse: 1.02208\n",
      "[1800]\tvalid_0's l2: 1.04457\tvalid_0's rmse: 1.02204\n",
      "[1900]\tvalid_0's l2: 1.04453\tvalid_0's rmse: 1.02202\n",
      "Early stopping, best iteration is:\n",
      "[1412]\tvalid_0's l2: 1.04355\tvalid_0's rmse: 1.02154\n",
      "Fold = 3. QWK = 0.48102216105532947.\n",
      "\n",
      "\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\tvalid_0's l2: 1.2689\tvalid_0's rmse: 1.12645\n",
      "[200]\tvalid_0's l2: 1.20021\tvalid_0's rmse: 1.09554\n",
      "[300]\tvalid_0's l2: 1.1707\tvalid_0's rmse: 1.08199\n",
      "[400]\tvalid_0's l2: 1.15368\tvalid_0's rmse: 1.07409\n",
      "[500]\tvalid_0's l2: 1.1465\tvalid_0's rmse: 1.07075\n",
      "[600]\tvalid_0's l2: 1.14222\tvalid_0's rmse: 1.06875\n",
      "[700]\tvalid_0's l2: 1.14091\tvalid_0's rmse: 1.06813\n",
      "[800]\tvalid_0's l2: 1.13793\tvalid_0's rmse: 1.06674\n",
      "[900]\tvalid_0's l2: 1.13707\tvalid_0's rmse: 1.06633\n",
      "[1000]\tvalid_0's l2: 1.1358\tvalid_0's rmse: 1.06574\n",
      "[1100]\tvalid_0's l2: 1.13512\tvalid_0's rmse: 1.06542\n",
      "[1200]\tvalid_0's l2: 1.13558\tvalid_0's rmse: 1.06563\n",
      "[1300]\tvalid_0's l2: 1.13574\tvalid_0's rmse: 1.06571\n",
      "[1400]\tvalid_0's l2: 1.13575\tvalid_0's rmse: 1.06572\n",
      "[1500]\tvalid_0's l2: 1.13586\tvalid_0's rmse: 1.06577\n",
      "[1600]\tvalid_0's l2: 1.13645\tvalid_0's rmse: 1.06604\n",
      "Early stopping, best iteration is:\n",
      "[1121]\tvalid_0's l2: 1.13456\tvalid_0's rmse: 1.06516\n",
      "Fold = 4. QWK = 0.43757145760817096.\n",
      "\n",
      "\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\tvalid_0's l2: 1.25907\tvalid_0's rmse: 1.12209\n",
      "[200]\tvalid_0's l2: 1.19304\tvalid_0's rmse: 1.09226\n",
      "[300]\tvalid_0's l2: 1.16646\tvalid_0's rmse: 1.08003\n",
      "[400]\tvalid_0's l2: 1.15669\tvalid_0's rmse: 1.0755\n",
      "[500]\tvalid_0's l2: 1.15089\tvalid_0's rmse: 1.0728\n",
      "[600]\tvalid_0's l2: 1.14694\tvalid_0's rmse: 1.07095\n",
      "[700]\tvalid_0's l2: 1.14347\tvalid_0's rmse: 1.06933\n",
      "[800]\tvalid_0's l2: 1.14217\tvalid_0's rmse: 1.06873\n",
      "[900]\tvalid_0's l2: 1.1402\tvalid_0's rmse: 1.0678\n",
      "[1000]\tvalid_0's l2: 1.13933\tvalid_0's rmse: 1.06739\n",
      "[1100]\tvalid_0's l2: 1.13959\tvalid_0's rmse: 1.06752\n",
      "[1200]\tvalid_0's l2: 1.13881\tvalid_0's rmse: 1.06715\n",
      "[1300]\tvalid_0's l2: 1.13897\tvalid_0's rmse: 1.06723\n",
      "[1400]\tvalid_0's l2: 1.13839\tvalid_0's rmse: 1.06695\n",
      "[1500]\tvalid_0's l2: 1.13845\tvalid_0's rmse: 1.06698\n",
      "[1600]\tvalid_0's l2: 1.13827\tvalid_0's rmse: 1.0669\n",
      "[1700]\tvalid_0's l2: 1.1386\tvalid_0's rmse: 1.06705\n",
      "[1800]\tvalid_0's l2: 1.13805\tvalid_0's rmse: 1.0668\n",
      "[1900]\tvalid_0's l2: 1.13791\tvalid_0's rmse: 1.06673\n",
      "[2000]\tvalid_0's l2: 1.13812\tvalid_0's rmse: 1.06683\n",
      "[2100]\tvalid_0's l2: 1.13842\tvalid_0's rmse: 1.06697\n",
      "[2200]\tvalid_0's l2: 1.13868\tvalid_0's rmse: 1.06709\n",
      "[2300]\tvalid_0's l2: 1.13888\tvalid_0's rmse: 1.06718\n",
      "Early stopping, best iteration is:\n",
      "[1885]\tvalid_0's l2: 1.13775\tvalid_0's rmse: 1.06665\n",
      "Fold = 5. QWK = 0.45748180697245955.\n",
      "\n",
      "\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\tvalid_0's l2: 1.20535\tvalid_0's rmse: 1.09789\n",
      "[200]\tvalid_0's l2: 1.14367\tvalid_0's rmse: 1.06942\n",
      "[300]\tvalid_0's l2: 1.11674\tvalid_0's rmse: 1.05676\n",
      "[400]\tvalid_0's l2: 1.10387\tvalid_0's rmse: 1.05065\n",
      "[500]\tvalid_0's l2: 1.09761\tvalid_0's rmse: 1.04767\n",
      "[600]\tvalid_0's l2: 1.09391\tvalid_0's rmse: 1.0459\n",
      "[700]\tvalid_0's l2: 1.0909\tvalid_0's rmse: 1.04446\n",
      "[800]\tvalid_0's l2: 1.0896\tvalid_0's rmse: 1.04384\n",
      "[900]\tvalid_0's l2: 1.08994\tvalid_0's rmse: 1.044\n",
      "[1000]\tvalid_0's l2: 1.08961\tvalid_0's rmse: 1.04384\n",
      "[1100]\tvalid_0's l2: 1.0891\tvalid_0's rmse: 1.0436\n",
      "[1200]\tvalid_0's l2: 1.08878\tvalid_0's rmse: 1.04345\n",
      "[1300]\tvalid_0's l2: 1.08902\tvalid_0's rmse: 1.04356\n",
      "[1400]\tvalid_0's l2: 1.08926\tvalid_0's rmse: 1.04368\n",
      "[1500]\tvalid_0's l2: 1.09016\tvalid_0's rmse: 1.04411\n",
      "[1600]\tvalid_0's l2: 1.09014\tvalid_0's rmse: 1.0441\n",
      "[1700]\tvalid_0's l2: 1.09024\tvalid_0's rmse: 1.04415\n",
      "Early stopping, best iteration is:\n",
      "[1270]\tvalid_0's l2: 1.08858\tvalid_0's rmse: 1.04335\n",
      "Fold = 6. QWK = 0.4467067664273372.\n",
      "\n",
      "\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\tvalid_0's l2: 1.22201\tvalid_0's rmse: 1.10545\n",
      "[200]\tvalid_0's l2: 1.15546\tvalid_0's rmse: 1.07492\n",
      "[300]\tvalid_0's l2: 1.12345\tvalid_0's rmse: 1.05993\n",
      "[400]\tvalid_0's l2: 1.1025\tvalid_0's rmse: 1.05\n",
      "[500]\tvalid_0's l2: 1.09107\tvalid_0's rmse: 1.04454\n",
      "[600]\tvalid_0's l2: 1.08368\tvalid_0's rmse: 1.041\n",
      "[700]\tvalid_0's l2: 1.0789\tvalid_0's rmse: 1.0387\n",
      "[800]\tvalid_0's l2: 1.07702\tvalid_0's rmse: 1.03779\n",
      "[900]\tvalid_0's l2: 1.07492\tvalid_0's rmse: 1.03678\n",
      "[1000]\tvalid_0's l2: 1.07419\tvalid_0's rmse: 1.03643\n",
      "[1100]\tvalid_0's l2: 1.07351\tvalid_0's rmse: 1.0361\n",
      "[1200]\tvalid_0's l2: 1.07251\tvalid_0's rmse: 1.03562\n",
      "[1300]\tvalid_0's l2: 1.07214\tvalid_0's rmse: 1.03544\n",
      "[1400]\tvalid_0's l2: 1.07248\tvalid_0's rmse: 1.0356\n",
      "[1500]\tvalid_0's l2: 1.07136\tvalid_0's rmse: 1.03506\n",
      "[1600]\tvalid_0's l2: 1.07081\tvalid_0's rmse: 1.0348\n",
      "[1700]\tvalid_0's l2: 1.07029\tvalid_0's rmse: 1.03455\n",
      "[1800]\tvalid_0's l2: 1.07061\tvalid_0's rmse: 1.0347\n",
      "[1900]\tvalid_0's l2: 1.07029\tvalid_0's rmse: 1.03455\n",
      "[2000]\tvalid_0's l2: 1.07033\tvalid_0's rmse: 1.03457\n",
      "[2100]\tvalid_0's l2: 1.07041\tvalid_0's rmse: 1.03461\n",
      "[2200]\tvalid_0's l2: 1.0706\tvalid_0's rmse: 1.0347\n",
      "[2300]\tvalid_0's l2: 1.07096\tvalid_0's rmse: 1.03487\n",
      "[2400]\tvalid_0's l2: 1.07098\tvalid_0's rmse: 1.03488\n",
      "[2500]\tvalid_0's l2: 1.07164\tvalid_0's rmse: 1.0352\n",
      "Early stopping, best iteration is:\n",
      "[2037]\tvalid_0's l2: 1.07012\tvalid_0's rmse: 1.03447\n",
      "Fold = 7. QWK = 0.4782009700592159.\n",
      "\n",
      "\n",
      "Training until validation scores don't improve for 500 rounds.\n",
      "[100]\tvalid_0's l2: 1.24784\tvalid_0's rmse: 1.11707\n",
      "[200]\tvalid_0's l2: 1.18398\tvalid_0's rmse: 1.08811\n",
      "[300]\tvalid_0's l2: 1.15332\tvalid_0's rmse: 1.07393\n",
      "[400]\tvalid_0's l2: 1.13618\tvalid_0's rmse: 1.06592\n",
      "[500]\tvalid_0's l2: 1.12835\tvalid_0's rmse: 1.06224\n",
      "[600]\tvalid_0's l2: 1.12301\tvalid_0's rmse: 1.05972\n",
      "[700]\tvalid_0's l2: 1.11778\tvalid_0's rmse: 1.05725\n",
      "[800]\tvalid_0's l2: 1.11374\tvalid_0's rmse: 1.05534\n",
      "[900]\tvalid_0's l2: 1.11077\tvalid_0's rmse: 1.05393\n",
      "[1000]\tvalid_0's l2: 1.10807\tvalid_0's rmse: 1.05265\n",
      "[1100]\tvalid_0's l2: 1.10673\tvalid_0's rmse: 1.05201\n",
      "[1200]\tvalid_0's l2: 1.10528\tvalid_0's rmse: 1.05132\n",
      "[1300]\tvalid_0's l2: 1.10446\tvalid_0's rmse: 1.05093\n",
      "[1400]\tvalid_0's l2: 1.1037\tvalid_0's rmse: 1.05057\n",
      "[1500]\tvalid_0's l2: 1.10289\tvalid_0's rmse: 1.05019\n",
      "[1600]\tvalid_0's l2: 1.10197\tvalid_0's rmse: 1.04975\n",
      "[1700]\tvalid_0's l2: 1.10211\tvalid_0's rmse: 1.04981\n",
      "[1800]\tvalid_0's l2: 1.10183\tvalid_0's rmse: 1.04968\n",
      "[1900]\tvalid_0's l2: 1.10146\tvalid_0's rmse: 1.04951\n",
      "[2000]\tvalid_0's l2: 1.10107\tvalid_0's rmse: 1.04932\n",
      "[2100]\tvalid_0's l2: 1.10087\tvalid_0's rmse: 1.04922\n",
      "[2200]\tvalid_0's l2: 1.10064\tvalid_0's rmse: 1.04912\n",
      "[2300]\tvalid_0's l2: 1.10051\tvalid_0's rmse: 1.04905\n",
      "[2400]\tvalid_0's l2: 1.10034\tvalid_0's rmse: 1.04897\n",
      "[2500]\tvalid_0's l2: 1.10031\tvalid_0's rmse: 1.04895\n",
      "[2600]\tvalid_0's l2: 1.10017\tvalid_0's rmse: 1.04889\n",
      "[2700]\tvalid_0's l2: 1.10031\tvalid_0's rmse: 1.04896\n",
      "[2800]\tvalid_0's l2: 1.10019\tvalid_0's rmse: 1.0489\n",
      "[2900]\tvalid_0's l2: 1.09986\tvalid_0's rmse: 1.04874\n",
      "[3000]\tvalid_0's l2: 1.09968\tvalid_0's rmse: 1.04866\n",
      "[3100]\tvalid_0's l2: 1.09972\tvalid_0's rmse: 1.04867\n",
      "[3200]\tvalid_0's l2: 1.09972\tvalid_0's rmse: 1.04867\n",
      "[3300]\tvalid_0's l2: 1.09971\tvalid_0's rmse: 1.04867\n",
      "[3400]\tvalid_0's l2: 1.09962\tvalid_0's rmse: 1.04863\n",
      "[3500]\tvalid_0's l2: 1.09961\tvalid_0's rmse: 1.04862\n",
      "[3600]\tvalid_0's l2: 1.09952\tvalid_0's rmse: 1.04858\n",
      "[3700]\tvalid_0's l2: 1.09973\tvalid_0's rmse: 1.04868\n",
      "[3800]\tvalid_0's l2: 1.09974\tvalid_0's rmse: 1.04868\n",
      "[3900]\tvalid_0's l2: 1.0996\tvalid_0's rmse: 1.04862\n",
      "[4000]\tvalid_0's l2: 1.09956\tvalid_0's rmse: 1.0486\n",
      "[4100]\tvalid_0's l2: 1.09952\tvalid_0's rmse: 1.04858\n",
      "[4200]\tvalid_0's l2: 1.09967\tvalid_0's rmse: 1.04865\n",
      "[4300]\tvalid_0's l2: 1.09973\tvalid_0's rmse: 1.04868\n",
      "[4400]\tvalid_0's l2: 1.0996\tvalid_0's rmse: 1.04862\n",
      "[4500]\tvalid_0's l2: 1.09951\tvalid_0's rmse: 1.04858\n",
      "Early stopping, best iteration is:\n",
      "[4073]\tvalid_0's l2: 1.09946\tvalid_0's rmse: 1.04855\n",
      "Fold = 8. QWK = 0.4621368590355688.\n",
      "\n",
      "\n",
      "Mean Score: 0.4584277416217084. Std Dev: 0.019880021317209298. Mean Coeff: [1.42356607 2.09556572 2.50057822 2.89195493]\n"
     ]
    }
   ],
   "source": [
    "FOLDS = 8\n",
    "train_predictions = np.zeros((train.shape[0], 1))\n",
    "test_predictions = np.zeros((test.shape[0], 1))\n",
    "zero_test_predictions = np.zeros((test.shape[0], 1))\n",
    "\n",
    "# print(\"stratified k-folds\")\n",
    "# cv = StratifiedKFold(n_splits = FOLDS, random_state = 42, shuffle = False)\n",
    "# cv.get_n_splits(train, target)\n",
    "\n",
    "# print(\"stratified grouped k-folds\")\n",
    "cv = GroupKFold(n_splits = FOLDS)\n",
    "cv.get_n_splits(train, target, rescuerid)\n",
    "\n",
    "cv_scores = []\n",
    "fold = 1\n",
    "coefficients = np.zeros((FOLDS, 4))\n",
    "for train_idx, valid_idx in cv.split(train, target, rescuerid):\n",
    "    xtrain, xvalid = train.iloc[train_idx], train.iloc[valid_idx]\n",
    "    ytrain, yvalid = target.iloc[train_idx], target.iloc[valid_idx]\n",
    "    \n",
    "    lgb_params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'learning_rate': 0.006,\n",
    "            'subsample': .8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'n_estimators': 10000,\n",
    "            'silent': -1,\n",
    "            'verbose': -1,\n",
    "            'random_state': 420,\n",
    "            'bagging_fraction': 0.9212945843023237,\n",
    "            'bagging_freq': int(2.1100859370529492),\n",
    "            'feature_fraction': 0.6334740217238963,\n",
    "            'lambda_l2': 1.543309192604612,\n",
    "            'max_bin': int(32.46977068537903),\n",
    "            'max_depth': int(11.982021953762485),\n",
    "            'min_child_samples': int(44.96596724925662),\n",
    "            'min_child_weight': 0.5878240657385082,\n",
    "            'min_split_gain': 0.004619759404679957,\n",
    "            'num_leaves': int(146.73598418222304)\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMRegressor(**lgb_params)\n",
    "    model.fit(\n",
    "        xtrain, ytrain,\n",
    "        eval_set = [(xvalid, yvalid)],\n",
    "        eval_metric = 'rmse',\n",
    "        verbose = 100,\n",
    "        early_stopping_rounds = 500\n",
    "    )\n",
    "    valid_preds = model.predict(xvalid, num_iteration = model.best_iteration_)\n",
    "\n",
    "    optR = OptimizedRounder()\n",
    "    optR.fit(valid_preds, yvalid.values)\n",
    "    coefficients[fold - 1, :] = optR.coefficients()\n",
    "    valid_p = optR.predict(valid_preds, coefficients[fold-1,:])\n",
    "    \n",
    "    test_preds = model.predict(test, num_iteration = model.best_iteration_)\n",
    "\n",
    "    scr = metric_function(yvalid.values, valid_p)\n",
    "    cv_scores.append(scr)\n",
    "    print(\"Fold = {}. QWK = {}.\".format(fold, scr))\n",
    "    print(\"\\n\")\n",
    "    train_predictions[valid_idx] = valid_preds.reshape(-1, 1)\n",
    "    test_predictions += test_preds.reshape(-1, 1)\n",
    "    fold += 1\n",
    "test_predictions = test_predictions * 1./FOLDS\n",
    "print(\"Mean Score: {}. Std Dev: {}. Mean Coeff: {}\".format(np.mean(cv_scores), np.std(cv_scores), np.mean(coefficients, axis = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "088d74226313f378d1f6493f2530d3bcc7a2678a"
   },
   "outputs": [],
   "source": [
    "optR = OptimizedRounder()\n",
    "optR.fit(train_predictions.reshape(1,-1)[0], target.values)\n",
    "coefficients = optR.coefficients()\n",
    "\n",
    "# Manually adjust coefficients\n",
    "# coefficients_ = coefficients.copy()\n",
    "# coefficients_[0] = 1.645\n",
    "# coefficients_[1] = 2.115\n",
    "# coefficients_[3] = 2.84\n",
    "\n",
    "predictions = optR.predict(test_predictions, coefficients).astype(int)\n",
    "# predictions = [item for sublist in predictions for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "11cdc06cf3c0daa697c7a37c6cb7860aa4251781"
   },
   "outputs": [],
   "source": [
    "sample = pd.read_csv('../input/petfinder-adoption-prediction/test/sample_submission.csv')\n",
    "sample.AdoptionSpeed = predictions\n",
    "sample.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0a87759008c7816bb60051bafd26850f79048e16"
   },
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "5d7379e359f4b042603dd60ffa87d93584a211bf"
   },
   "outputs": [],
   "source": [
    "# random_grid = {\n",
    "#     'bootstrap': [True, False],\n",
    "#     'max_depth': [50, 85],\n",
    "#     'max_features': ['auto'],\n",
    "#     'min_samples_leaf': [10, 15],\n",
    "#     'min_samples_split': [10, 15],\n",
    "#     'n_estimators': [150, 200, 215],\n",
    "#     'random_state' : [seed]\n",
    "# }\n",
    "\n",
    "# rf = RandomForestClassifier()\n",
    "# # rf_random = RandomizedSearchCV(estimator = rf, \n",
    "# #                                param_distributions = random_grid, \n",
    "# #                                n_iter = 100,\n",
    "# #                                cv = 3,\n",
    "# #                                scoring = kappa_scorer,\n",
    "# #                                verbose = 2, \n",
    "# #                                random_state = 42, \n",
    "# #                                n_jobs = -1)\n",
    "# # rf_random.fit(train, target)\n",
    "# # print(rf_random.best_score_)\n",
    "# # print(rf_random.best_params_)\n",
    "\n",
    "# best_params = {'random_state': 42, \n",
    "#                'n_estimators': 215, \n",
    "#                'min_samples_split': 10, \n",
    "#                'min_samples_leaf': 10, \n",
    "#                'max_features': 'auto', \n",
    "#                'max_depth': 50, \n",
    "#                'bootstrap': False}\n",
    "# rf.set_params(**best_params)\n",
    "# rf.fit(train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "4f874838b3299956b1df5619bb4821b99f96889d"
   },
   "outputs": [],
   "source": [
    "# rf_val_score = np.mean(cross_val_score(rf, train, target, scoring = kappa_scorer, cv=5))\n",
    "# rf_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "48f4fbfffe3a693776177d60ad5970248292617d"
   },
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "f0c6a8e4e8e349353bce3d9a7435bd91dffa1bc6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create the random grid\n",
    "# random_grid = {'loss' : ['deviance'],\n",
    "#                 'learning_rate' : [.025, 0.5],\n",
    "#                 'max_depth': [5, 8],\n",
    "#                 'max_features': ['auto'],\n",
    "#                 'min_samples_leaf': [100],\n",
    "#                 'min_samples_split': [100],\n",
    "#                 'n_estimators': [100],\n",
    "#                 'subsample' : [.8],\n",
    "#                 'random_state' : [seed]\n",
    "#               }\n",
    "\n",
    "# gbm = GradientBoostingClassifier()\n",
    "# gbm_random = RandomizedSearchCV(estimator = gbm, \n",
    "#                    param_distributions = random_grid, \n",
    "#                    n_iter = 100,\n",
    "#                    cv = 3,\n",
    "#                    scoring = kappa_scorer,\n",
    "#                    verbose = 2, \n",
    "#                    random_state = 42, \n",
    "#                    n_jobs = -1)\n",
    "# gbm_random.fit(train, target)\n",
    "\n",
    "# best_params = {'subsample': 0.8, \n",
    "#                'random_state': 42, \n",
    "#                'n_estimators': 100, \n",
    "#                'min_samples_split': 100, \n",
    "#                'min_samples_leaf': 100, \n",
    "#                'max_features': 'auto', \n",
    "#                'max_depth': 8, \n",
    "#                'loss': 'deviance', \n",
    "#                'learning_rate': 0.025}\n",
    "# gbm.set_params(**best_params)\n",
    "# gbm.fit(train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "_uuid": "916af40a7551ceb696da0243856ec02753cdbd1d"
   },
   "outputs": [],
   "source": [
    "# gbm_val_score = np.mean(cross_val_score(gbm, train, target, scoring = kappa_scorer, cv=5))\n",
    "# gbm_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "109a876f600e90133778a2c88c9f2c39b63412b5"
   },
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_uuid": "a59635aac2431be4c301eaf733583d0b32583ef7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Parameters\n",
    "# xgb_params = {'objective' : 'multi:softmax',\n",
    "#               'eval_metric' : 'mlogloss',\n",
    "#               'eta' : 0.05,\n",
    "#               'max_depth' : 4,\n",
    "#               'num_class' : 5,\n",
    "#               'lambda' : 0.8\n",
    "# }\n",
    "\n",
    "# d_train = xgb.DMatrix(train, label = target)\n",
    "# d_val = xgb.DMatrix(x_val, label = y_val)\n",
    "\n",
    "# watchlist = [(d_train, 'train'), (d_val, 'valid')]\n",
    "\n",
    "# xgb_model = xgb.train(xgb_params, \n",
    "#                 d_train, \n",
    "#                 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_uuid": "c1f7489c359fa935ed0813fdd6a8dc0980c41c36"
   },
   "outputs": [],
   "source": [
    "# metric_function(xgb_model.predict(xgb.DMatrix(x_val)).astype(int), y_val)\n",
    "# dtest = xgb.DMatrix(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d2827443012abea15de2eeffe63ece953ad65bd6"
   },
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_uuid": "95c34cd0df659170c56f34ea09965b46cd3e3754"
   },
   "outputs": [],
   "source": [
    "# submission_df = pd.DataFrame(data={'PetID' : test_orig['PetID'], \n",
    "#                                    'AdoptionSpeed' : xgb_model.predict(dtest).astype(int)})\n",
    "# submission_df.to_csv('submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "_uuid": "a254e815b97ea0f7fb90109e92c1146da0459dcb"
   },
   "outputs": [],
   "source": [
    "# submission_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

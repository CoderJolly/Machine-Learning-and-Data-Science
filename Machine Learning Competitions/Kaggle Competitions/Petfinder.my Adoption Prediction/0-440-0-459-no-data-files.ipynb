{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import cv2\n",
    "from datetime import datetime, timedelta, date\n",
    "import numpy as np\n",
    "import warnings\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import pprint\n",
    "import string\n",
    "from glob import glob\n",
    "from numpy import nanmean\n",
    "from functools import partial\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GroupKFold, RandomizedSearchCV, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import lightgbm as lgb\n",
    "import scipy as sp\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from keras.applications.densenet import preprocess_input, DenseNet121\n",
    "from keras.models import Model\n",
    "from keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\n",
    "import keras.backend as K\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import make_scorer, cohen_kappa_score, log_loss, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "\n",
    "# For text processing\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.layers import Input, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# stop_words = []\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "warnings.filterwarnings('ignore')\n",
    "punctuation = string.punctuation\n",
    "\n",
    "pd.set_option('max_columns', 50)\n",
    "pd.options.mode.chained_assignment = None\n",
    "pp = pprint.PrettyPrinter(indent = 4)\n",
    "\n",
    "import random\n",
    "random.seed(420)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_orig = pd.read_csv('../input/petfinder-adoption-prediction/train/train.csv')\n",
    "test_orig = pd.read_csv('../input/petfinder-adoption-prediction/test/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d5823356caa8c49a4f608c26df447c7314bc003b"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "e8ea3930e4fb43649e8a83b23bf5576a181c698a"
   },
   "outputs": [],
   "source": [
    "def get_description_sentiment(df, df_type = \"train\"):\n",
    "    doc_sent_mag = []\n",
    "    doc_sent_score = []\n",
    "    nf_count = 0\n",
    "    for pet_id in df.PetID.values:\n",
    "        try:\n",
    "            with open('../input/petfinder-adoption-prediction/' + df_type + '_sentiment/' + pet_id + '.json', 'r') as f:\n",
    "                sentiment = json.load(f)\n",
    "            doc_sent_mag.append(sentiment['documentSentiment']['magnitude'])\n",
    "            doc_sent_score.append(sentiment['documentSentiment']['score'])\n",
    "        except FileNotFoundError:\n",
    "            nf_count += 1\n",
    "            doc_sent_mag.append(-1)\n",
    "            doc_sent_score.append(-1)\n",
    "            \n",
    "    df['doc_sent_mag'] = doc_sent_mag\n",
    "    df['doc_sent_score'] = doc_sent_score\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "a53fa39250eeecd2571e9639dc902a818804bb6d"
   },
   "outputs": [],
   "source": [
    "def get_image_metadata(df, df_type = \"train\"):\n",
    "    vertex_xs = []\n",
    "    vertex_ys = []\n",
    "    bounding_confidences = []\n",
    "    bounding_importance_fracs = []\n",
    "    dominant_blues = []\n",
    "    dominant_greens = []\n",
    "    dominant_reds = []\n",
    "    dominant_pixel_fracs = []\n",
    "    dominant_scores = []\n",
    "    label_scores = []\n",
    "    nf_count = 0\n",
    "    nl_count = 0\n",
    "    \n",
    "    pets_metadata = {}\n",
    "    for file_name in os.listdir('../input/petfinder-adoption-prediction/' + df_type + '_metadata/'):\n",
    "        pet_id = file_name.split('-')[0]\n",
    "        if pet_id not in pets_metadata:\n",
    "            pets_metadata[pet_id] = []\n",
    "        pets_metadata[pet_id].append(file_name)\n",
    "    \n",
    "    for pet_id in df.PetID.values:\n",
    "        if pet_id in pets_metadata:\n",
    "            pet_id_metadata_files = pets_metadata[pet_id]\n",
    "            \n",
    "            temp_vertex_xs = []\n",
    "            temp_vertex_ys = []\n",
    "            temp_bounding_confidences = []\n",
    "            temp_bounding_importance_fracs = []\n",
    "            temp_dominant_blues = []\n",
    "            temp_dominant_greens = []\n",
    "            temp_dominant_reds = []\n",
    "            temp_dominant_pixel_fracs = []\n",
    "            temp_dominant_scores = []\n",
    "            temp_label_scores = []\n",
    "            for file in pet_id_metadata_files:\n",
    "                with open('../input/petfinder-adoption-prediction/' + df_type + '_metadata/' + file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                \n",
    "                try:\n",
    "                    vertex_x = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['x']\n",
    "                    temp_vertex_xs.append(vertex_x)\n",
    "                except:\n",
    "                    temp_vertex_xs.append(-1)\n",
    "                \n",
    "                try:\n",
    "                    vertex_y = data['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]['y']\n",
    "                    temp_vertex_ys.append(vertex_y)\n",
    "                except:\n",
    "                    temp_vertex_ys.append(-1)\n",
    "                \n",
    "                try:\n",
    "                    bounding_confidence = data['cropHintsAnnotation']['cropHints'][0]['confidence']\n",
    "                    temp_bounding_confidences.append(bounding_confidence)\n",
    "                except:\n",
    "                    temp_bounding_confidences.append(-1)\n",
    "                \n",
    "                try:\n",
    "                    bounding_importance_frac = data['cropHintsAnnotation']['cropHints'][0].get('importanceFraction', -1)\n",
    "                    temp_bounding_importance_fracs.append(bounding_importance_frac)\n",
    "                except:\n",
    "                    temp_bounding_importance_fracs.append(-1)\n",
    "                \n",
    "                try:\n",
    "                    dominant_blue = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['blue']\n",
    "                    temp_dominant_blues.append(dominant_blue)\n",
    "                except:\n",
    "                    temp_dominant_blues.append(-1)\n",
    "                \n",
    "                try:\n",
    "                    dominant_green = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['green']\n",
    "                    temp_dominant_greens.append(dominant_green)\n",
    "                except:\n",
    "                    temp_dominant_greens.append(-1)\n",
    "                \n",
    "                try:\n",
    "                    dominant_red = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['color']['red']\n",
    "                    temp_dominant_reds.append(dominant_red)\n",
    "                except:\n",
    "                    temp_dominant_reds.append(-1)\n",
    "                \n",
    "                try:\n",
    "                    dominant_pixel_frac = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['pixelFraction']\n",
    "                    temp_dominant_pixel_fracs.append(dominant_pixel_frac)\n",
    "                except:\n",
    "                    temp_dominant_pixel_fracs.append(-1)\n",
    "                \n",
    "                try:\n",
    "                    dominant_score = data['imagePropertiesAnnotation']['dominantColors']['colors'][0]['score']\n",
    "                    temp_dominant_scores.append(dominant_score)\n",
    "                except:\n",
    "                    temp_dominant_scores.append(-1)\n",
    "\n",
    "                if data.get('labelAnnotations'):\n",
    "                    label_score = data['labelAnnotations'][0]['score']\n",
    "                    temp_label_scores.append(label_score)\n",
    "                else:\n",
    "                    temp_label_scores.append(-1)\n",
    "            \n",
    "            vertex_xs.append(np.mean(temp_vertex_xs))\n",
    "            vertex_ys.append(np.mean(temp_vertex_ys))\n",
    "            bounding_confidences.append(np.mean(temp_bounding_confidences))\n",
    "            bounding_importance_fracs.append(np.mean(temp_bounding_importance_fracs))\n",
    "            dominant_blues.append(np.mean(temp_dominant_blues))\n",
    "            dominant_greens.append(np.mean(temp_dominant_greens))\n",
    "            dominant_reds.append(np.mean(temp_dominant_reds))\n",
    "            dominant_pixel_fracs.append(np.mean(temp_dominant_pixel_fracs))\n",
    "            dominant_scores.append(np.mean(temp_dominant_scores))\n",
    "            label_scores.append(np.mean(temp_label_scores))\n",
    "        else:\n",
    "            nf_count += 1\n",
    "            vertex_xs.append(-1)\n",
    "            vertex_ys.append(-1)\n",
    "            bounding_confidences.append(-1)\n",
    "            bounding_importance_fracs.append(-1)\n",
    "            dominant_blues.append(-1)\n",
    "            dominant_greens.append(-1)\n",
    "            dominant_reds.append(-1)\n",
    "            dominant_pixel_fracs.append(-1)\n",
    "            dominant_scores.append(-1)\n",
    "            label_scores.append(-1)\n",
    "            \n",
    "    df.loc[:, 'vertex_x'] = vertex_xs\n",
    "    df.loc[:, 'vertex_y'] = vertex_ys\n",
    "    df.loc[:, 'bounding_confidence'] = bounding_confidences\n",
    "    df.loc[:, 'bounding_importance'] = bounding_importance_fracs\n",
    "    df.loc[:, 'dominant_blue'] = dominant_blues\n",
    "    df.loc[:, 'dominant_green'] = dominant_greens\n",
    "    df.loc[:, 'dominant_red'] = dominant_reds\n",
    "    df.loc[:, 'dominant_pixel_frac'] = dominant_pixel_fracs\n",
    "    df.loc[:, 'dominant_score'] = dominant_scores\n",
    "    df.loc[:, 'label_score'] = label_scores\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Textual Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "24300b30525748ad1ac15dbda50c56f44c11a9ea"
   },
   "outputs": [],
   "source": [
    "# Reference - https://www.kaggle.com/shivamb/extensive-text-data-feature-engineering\n",
    "\n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 95000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 1257 # max number of words in a description to use\n",
    "\n",
    "pos_dic = {\n",
    "    'noun' : ['NN','NNS','NNP','NNPS'],\n",
    "    'pron' : ['PRP','PRP$','WP','WP$'],\n",
    "    'verb' : ['VB','VBD','VBG','VBN','VBP','VBZ'],\n",
    "    'adj' :  ['JJ','JJR','JJS'],\n",
    "    'adv' : ['RB','RBR','RBS','WRB']\n",
    "}\n",
    "\n",
    "puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '•',  '~', '@', '£', \n",
    " '·', '_', '{', '}', '©', '^', '®', '`',  '<', '→', '°', '€', '™', '›',  '♥', '←', '×', '§', '″', '′', 'Â', '█', '½', 'à', '…', \n",
    " '“', '★', '”', '–', '●', 'â', '►', '−', '¢', '²', '¬', '░', '¶', '↑', '±', '¿', '▾', '═', '¦', '║', '―', '¥', '▓', '—', '‹', '─', \n",
    " '▒', '：', '¼', '⊕', '▼', '▪', '†', '■', '’', '▀', '¨', '▄', '♫', '☆', 'é', '¯', '♦', '¤', '▲', 'è', '¸', '¾', 'Ã', '⋅', '‘', '∞', \n",
    " '∙', '）', '↓', '、', '│', '（', '»', '，', '♪', '╩', '╚', '³', '・', '╦', '╣', '╔', '╗', '▬', '❤', 'ï', 'Ø', '¹', '≤', '‡', '√', ]\n",
    "\n",
    "mispell_dict = {\"aren't\" : \"are not\",\n",
    "                \"can't\" : \"cannot\",\n",
    "                \"couldn't\" : \"could not\",\n",
    "                \"didn't\" : \"did not\",\n",
    "                \"doesn't\" : \"does not\",\n",
    "                \"don't\" : \"do not\",\n",
    "                \"hadn't\" : \"had not\",\n",
    "                \"hasn't\" : \"has not\",\n",
    "                \"haven't\" : \"have not\",\n",
    "                \"he'd\" : \"he would\",\n",
    "                \"he'll\" : \"he will\",\n",
    "                \"he's\" : \"he is\",\n",
    "                \"i'd\" : \"I would\",\n",
    "                \"i'd\" : \"I had\",\n",
    "                \"i'll\" : \"I will\",\n",
    "                \"i'm\" : \"I am\",\n",
    "                \"isn't\" : \"is not\",\n",
    "                \"it's\" : \"it is\",\n",
    "                \"it'll\":\"it will\",\n",
    "                \"i've\" : \"I have\",\n",
    "                \"let's\" : \"let us\",\n",
    "                \"mightn't\" : \"might not\",\n",
    "                \"mustn't\" : \"must not\",\n",
    "                \"shan't\" : \"shall not\",\n",
    "                \"she'd\" : \"she would\",\n",
    "                \"she'll\" : \"she will\",\n",
    "                \"she's\" : \"she is\",\n",
    "                \"shouldn't\" : \"should not\",\n",
    "                \"that's\" : \"that is\",\n",
    "                \"there's\" : \"there is\",\n",
    "                \"they'd\" : \"they would\",\n",
    "                \"they'll\" : \"they will\",\n",
    "                \"they're\" : \"they are\",\n",
    "                \"they've\" : \"they have\",\n",
    "                \"we'd\" : \"we would\",\n",
    "                \"we're\" : \"we are\",\n",
    "                \"weren't\" : \"were not\",\n",
    "                \"we've\" : \"we have\",\n",
    "                \"what'll\" : \"what will\",\n",
    "                \"what're\" : \"what are\",\n",
    "                \"what's\" : \"what is\",\n",
    "                \"what've\" : \"what have\",\n",
    "                \"where's\" : \"where is\",\n",
    "                \"who'd\" : \"who would\",\n",
    "                \"who'll\" : \"who will\",\n",
    "                \"who're\" : \"who are\",\n",
    "                \"who's\" : \"who is\",\n",
    "                \"who've\" : \"who have\",\n",
    "                \"won't\" : \"will not\",\n",
    "                \"wouldn't\" : \"would not\",\n",
    "                \"you'd\" : \"you would\",\n",
    "                \"you'll\" : \"you will\",\n",
    "                \"you're\" : \"you are\",\n",
    "                \"you've\" : \"you have\",\n",
    "                \"'re\": \" are\",\n",
    "                \"wasn't\": \"was not\",\n",
    "                \"we'll\":\" will\",\n",
    "                \"didn't\": \"did not\",\n",
    "                \"tryin'\":\"trying\"\n",
    "}\n",
    "\n",
    "def clean_text(x):\n",
    "    x = str(x)\n",
    "    for punct in puncts:\n",
    "        x = x.replace(punct, f' {punct} ')\n",
    "    return x\n",
    "\n",
    "def clean_numbers(x):\n",
    "    x = re.sub('[0-9]{5,}', '#####', x)\n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "def _get_mispell(mispell_dict):\n",
    "    mispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    return mispell_dict, mispell_re\n",
    "\n",
    "mispellings, mispellings_re = _get_mispell(mispell_dict)\n",
    "def replace_typical_misspell(text):\n",
    "    def replace(match):\n",
    "        return mispellings[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "def get_polarity(text):\n",
    "    try:\n",
    "        textblob = TextBlob(unicode(text, 'utf-8'))\n",
    "        pol = textblob.sentiment.polarity\n",
    "    except:\n",
    "        pol = 0.0\n",
    "    return pol\n",
    "\n",
    "def get_subjectivity(text):\n",
    "    try:\n",
    "        textblob = TextBlob(unicode(text, 'utf-8'))\n",
    "        subj = textblob.sentiment.subjectivity\n",
    "    except:\n",
    "        subj = 0.0\n",
    "    return subj\n",
    "\n",
    "def get_parts_of_speech_count(x):\n",
    "    cnt = {\n",
    "        'noun': 0,\n",
    "        'pron': 0,\n",
    "        'verb': 0,\n",
    "        'adj': 0,\n",
    "        'adv': 0\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        wiki = TextBlob(x)\n",
    "        for tup in wiki.tags:\n",
    "            ppo = list(tup)[1]\n",
    "            for pos_tag_type in cnt.keys():\n",
    "                if ppo in pos_dic[pos_tag_type]:\n",
    "                    cnt[pos_tag_type] += 1\n",
    "    except:\n",
    "        pass\n",
    "    return [cnt[i] for i in cnt.keys()]\n",
    "\n",
    "def sent2vec(description, embedding_index_1, embedding_index_2 = None):\n",
    "    M = []\n",
    "    for w in word_tokenize(description):\n",
    "        if not w.isalpha():\n",
    "            continue\n",
    "        if w in embedding_index_1 and w in embedding_index_2:\n",
    "            embedding_vector = np.mean([embedding_index_1[w], embedding_index_2[w]], axis = 0)\n",
    "            M.append(embedding_vector)\n",
    "            continue\n",
    "        if w in embedding_index_1:\n",
    "            embedding_vector = embedding_index_1[w]\n",
    "            M.append(embedding_vector)\n",
    "            continue\n",
    "        if w in embedding_index_2:\n",
    "            embedding_vector = embedding_index_2[w]\n",
    "            M.append(embedding_vector)\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis = 0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "def get_textual_features_from_description(df):\n",
    "    \n",
    "    # Fill NA\n",
    "    df[[\"Description\"]] = df[[\"Description\"]].fillna(\"none\")\n",
    "    \n",
    "    # Get stopwords count\n",
    "    print(\"Get stopword count\")\n",
    "    df['Description_stopword_count'] = df['Description'].apply(lambda x: len([wrd for wrd in x.split() if wrd.lower() in stop_words]))\n",
    "    \n",
    "    # Convert to lower\n",
    "    print(\"Convert to lower\")\n",
    "    df[\"Description\"] = df[\"Description\"].apply(lambda x: x.lower())\n",
    "    \n",
    "    # Clean the text\n",
    "    print(\"Clean the text\")\n",
    "    df[\"Description\"] = df[\"Description\"].apply(lambda x: clean_text(x))\n",
    "    \n",
    "    # Clean spellings\n",
    "    print(\"Convert the spellings\")\n",
    "    df[\"Description\"] = df[\"Description\"].apply(lambda x: replace_typical_misspell(x))\n",
    "    \n",
    "    # Remove stopwords\n",
    "    df[\"Description\"] = df[\"Description\"].apply(lambda x: \" \".join([item for item in x if item not in stop_words]))\n",
    "    \n",
    "    # Get character count\n",
    "    print(\"Get character count\")\n",
    "    df['Description_character_count'] = df['Description'].str.len()\n",
    "    \n",
    "    # Get word count\n",
    "    print(\"Get subjectivity\")\n",
    "    df['Description_word_count'] = df['Description'].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    # Get word density\n",
    "    print(\"Get word density\")\n",
    "    df['Description_word_density'] = df['Description_character_count']/(df['Description_word_count'] + 1)\n",
    "    \n",
    "    # Get number of punctuations in a description\n",
    "    print(\"Get punctuation count\")\n",
    "    df['Description_punctuation_count'] = df['Description'].apply(lambda x: len(\"\".join(_ for _ in x if _ in punctuation)))\n",
    "    \n",
    "    # Get polarity\n",
    "    print(\"Get polarity\")\n",
    "    df['Description_polarity'] = df['Description'].apply(get_polarity)\n",
    "    \n",
    "    # Get subjectivity\n",
    "    print(\"Get subjectivity\")\n",
    "    df['Description_subjectivity'] = df['Description'].apply(get_subjectivity)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Image Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "b999199c2bcb595f2e23e6abd225dca8e7edef29"
   },
   "outputs": [],
   "source": [
    "# Reference - https://www.kaggle.com/christofhenkel/extract-image-features-from-pretrained-nn/notebook\n",
    "img_size = 256\n",
    "batch_size = 16\n",
    "\n",
    "def resize_to_square(im):\n",
    "    old_size = im.shape[:2] # old_size is in (height, width) format\n",
    "    ratio = float(img_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    \n",
    "    # new_size should be in (width, height) format\n",
    "    im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "    delta_w = img_size - new_size[1]\n",
    "    delta_h = img_size - new_size[0]\n",
    "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "    color = [0, 0, 0]\n",
    "    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n",
    "    return new_im\n",
    "\n",
    "def load_image(path, pet_id):\n",
    "    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n",
    "    new_image = resize_to_square(image)\n",
    "    new_image = preprocess_input(new_image)\n",
    "    return new_image\n",
    "\n",
    "def get_raw_image_features(df, df_type = \"train\"):\n",
    "    pet_ids = df['PetID'].values\n",
    "    n_batches = len(pet_ids) // batch_size + 1\n",
    "    \n",
    "    inp = Input((256,256,3))\n",
    "    backbone = DenseNet121(input_tensor = inp, include_top = False, weights = '../input/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "    x = backbone.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\n",
    "    x = AveragePooling1D(4)(x)\n",
    "    out = Lambda(lambda x: x[:,:,0])(x)\n",
    "\n",
    "    m = Model(inp, out)\n",
    "    \n",
    "    features = {}\n",
    "    for b in tqdm_notebook(range(n_batches)):\n",
    "        start = b*batch_size\n",
    "        end = (b+1)*batch_size\n",
    "        batch_pets = pet_ids[start:end]\n",
    "        batch_images = np.zeros((len(batch_pets), img_size, img_size, 3))\n",
    "        for i,pet_id in enumerate(batch_pets):\n",
    "            try:\n",
    "                batch_images[i] = load_image(\"../input/petfinder-adoption-prediction/\" + df_type + \"_images/\", pet_id)\n",
    "            except:\n",
    "                pass\n",
    "        batch_preds = m.predict(batch_images)\n",
    "        for i, pet_id in enumerate(batch_pets):\n",
    "            features[pet_id] = batch_preds[i]\n",
    "            \n",
    "    feats_df = pd.DataFrame.from_dict(features, orient = 'index').reset_index()\n",
    "    feats_df['PetID'] = feats_df['index']\n",
    "    feats_df = feats_df.drop('index', 1)\n",
    "    df = pd.merge(df, feats_df, how = 'left', on = ['PetID'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregate Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "c30180b140ab8cacbf4099c6807382ffe81de9e8"
   },
   "outputs": [],
   "source": [
    "def get_aggregate_features(df):\n",
    "    \n",
    "#     # Create dummies\n",
    "#     df = pd.get_dummies(df, columns = ['Vaccinated', 'Dewormed', 'Sterilized', 'Health'])\n",
    "    \n",
    "    # Some RescuerID aggregate features\n",
    "    aggs = {}\n",
    "    aggs['Fee'] = ['mean', 'std', 'min', 'max']\n",
    "    aggs['Quantity'] = ['mean', 'std', 'min', 'max']\n",
    "    aggs['Age'] = ['mean', 'std', 'min', 'max']\n",
    "    aggs['PhotoAmt'] = ['mean', 'std', 'min', 'max']\n",
    "    aggs['VideoAmt'] = ['mean', 'std', 'min', 'max']\n",
    "    aggs['FurLength'] = ['mean', 'std', 'min', 'max']\n",
    "    aggs['MaturitySize'] = ['mean', 'std', 'min', 'max']\n",
    "    \n",
    "    for groupby_col in ['RescuerID', 'State', 'Gender']:\n",
    "        df_ = df.reset_index().groupby(groupby_col).agg(aggs)\n",
    "        df_.columns = [groupby_col + '_' + '_'.join(col).strip() for col in df_.columns.values]\n",
    "        df_.reset_index(inplace = True)\n",
    "        df = pd.merge(df, df_, on = [groupby_col], how = 'left')\n",
    "    \n",
    "    df_ = df.reset_index().groupby(['RescuerID', 'State']).agg(aggs)\n",
    "    df_.columns = ['Rescuer_State' + '_' + '_'.join(col).strip() for col in df_.columns.values]\n",
    "    df_.reset_index(inplace = True)\n",
    "    df = pd.merge(df, df_, on = ['RescuerID', 'State'], how = 'left')\n",
    "    \n",
    "    df_ = df.reset_index().groupby(['RescuerID', 'Gender']).agg(aggs)\n",
    "    df_.columns = ['Rescuer_Gender' + '_' + '_'.join(col).strip() for col in df_.columns.values]\n",
    "    df_.reset_index(inplace = True)\n",
    "    df = pd.merge(df, df_, on = ['RescuerID', 'Gender'], how = 'left')\n",
    "        \n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "d6a16073141c3a2f575abbf087de55b1dd5a783b"
   },
   "outputs": [],
   "source": [
    "def gv_rel_text_area_feature(im_meta):\n",
    "    text_area = 0\n",
    "    try:\n",
    "        for textbox in im_meta['textAnnotations']:\n",
    "            try:\n",
    "                rect = np.array([[c['x'], c['y']] for c in textbox['boundingPoly']['vertices']])\n",
    "                bl = np.min(rect, axis=0)\n",
    "                size = np.max(rect, axis=0) - bl\n",
    "                text_area += np.prod(size)\n",
    "            except KeyError:\n",
    "                pass\n",
    "    except KeyError:\n",
    "        pass\n",
    "        \n",
    "    try:\n",
    "        imsize = im_meta['cropHintsAnnotation']['cropHints'][0]['boundingPoly']['vertices'][2]\n",
    "        im_area = np.prod([int(imsize['y']+1), int(imsize['x']+1)])\n",
    "    except (KeyError, TypeError):\n",
    "        return 0\n",
    "    return text_area/im_area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rescuer Experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "f731d830f1b9bd9636c5bfae78f460fd81ad052a"
   },
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "\n",
    "def get_rescuer_experience(d_train, d_test):\n",
    "    \"\"\" Creates rescuer experience feature vector.\n",
    "    Vector of crossproduct of Type and maturity size (8 elements).\n",
    "    Each element has the experience in that category of the rescuer,\n",
    "    normalized to the total experience in the total (train+test) dataset.\n",
    "    Returns:\n",
    "        Nothing, columns added to the Dataframes\n",
    "    \"\"\"\n",
    "    \n",
    "    d = d_train.append(d_test, sort=True)\n",
    "    feature = np.zeros((len(d), 8), np.float32)\n",
    "    types = d.Type.map(str)\n",
    "    maturities = d.MaturitySize.map(str)\n",
    "    h = Counter(d.RescuerID.values + \"_\" + types + \"_\" + maturities)\n",
    "    keys = [f\"_{i}_{j}\" for i,j in product(set(types), set(maturities))]\n",
    "    for i, rid in enumerate(d['RescuerID']):\n",
    "        feature[i] = [h[rid+k] for k in keys]\n",
    "    for i, k in enumerate(keys):\n",
    "        d_train[\"rescuer\" + k] = feature[0:len(d_train),i]\n",
    "        d_test[\"rescuer\" + k] = feature[len(d_train):,i]\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svd_features(train, test):\n",
    "    tfv = TfidfVectorizer(min_df=3,  max_features=10000,\n",
    "        strip_accents='unicode', analyzer='word', token_pattern=r'\\w{1,}',\n",
    "        ngram_range=(1, 3), use_idf=1, smooth_idf=1, sublinear_tf=1,\n",
    "        stop_words = 'english')\n",
    "\n",
    "    train_desc = train[\"Description\"].fillna(\"none\").values\n",
    "    test_desc = test[\"Description\"].fillna(\"none\").values\n",
    "\n",
    "    tfv.fit(list(train_desc) + list(test_desc))\n",
    "    X_train =  tfv.transform(train_desc)\n",
    "    X_test = tfv.transform(test_desc)\n",
    "\n",
    "    svd = TruncatedSVD(n_components = 120, random_state = 420)\n",
    "    svd.fit(X_train)\n",
    "    X_train = svd.transform(X_train)\n",
    "    X_test = svd.transform(X_test)\n",
    "\n",
    "    X_train = pd.DataFrame(X_train, columns=['svd_{}'.format(i) for i in range(120)])\n",
    "    X_test = pd.DataFrame(X_test, columns=['svd_{}'.format(i) for i in range(120)])\n",
    "    train = pd.concat((train, X_train), axis = 1)\n",
    "    test = pd.concat((test, X_test), axis = 1)\n",
    "    \n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state GDP: https://en.wikipedia.org/wiki/List_of_Malaysian_states_by_GDP\n",
    "state_gdp = {\n",
    "    41336: 116.679,\n",
    "    41325: 40.596,\n",
    "    41367: 23.02,\n",
    "    41401: 190.075,\n",
    "    41415: 5.984,\n",
    "    41324: 37.274,\n",
    "    41332: 42.389,\n",
    "    41335: 52.452,\n",
    "    41330: 67.629,\n",
    "    41380: 5.642,\n",
    "    41327: 81.284,\n",
    "    41345: 80.167,\n",
    "    41342: 121.414,\n",
    "    41326: 280.698,\n",
    "    41361: 32.270\n",
    "}\n",
    "\n",
    "# state population: https://en.wikipedia.org/wiki/Malaysia\n",
    "state_population = {\n",
    "    41336: 33.48283,\n",
    "    41325: 19.47651,\n",
    "    41367: 15.39601,\n",
    "    41401: 16.74621,\n",
    "    41415: 0.86908,\n",
    "    41324: 8.21110,\n",
    "    41332: 10.21064,\n",
    "    41335: 15.00817,\n",
    "    41330: 23.52743,\n",
    "    41380: 2.31541,\n",
    "    41327: 15.61383,\n",
    "    41345: 32.06742,\n",
    "    41342: 24.71140,\n",
    "    41326: 54.62141,\n",
    "    41361: 10.35977\n",
    "}\n",
    "\n",
    "state_area = {\n",
    "    41336:19102,\n",
    "    41325:9500,\n",
    "    41367:15099,\n",
    "    41401:243,\n",
    "    41415:91,\n",
    "    41324:1664,\n",
    "    41332:6686,\n",
    "    41335:36137,\n",
    "    41330:21035,\n",
    "    41380:821,\n",
    "    41327:1048,\n",
    "    41345:73631,\n",
    "    41342:124450,\n",
    "    41326:8104,\n",
    "    41361:13035\n",
    "}\n",
    "\n",
    "def get_state_features(df):\n",
    "    df[\"state_gdp\"] = df.State.map(state_gdp)\n",
    "    df[\"state_population\"] = df.State.map(state_population)\n",
    "    df[\"state_area\"] = df.State.map(state_area)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "f9230db38c6457cc924092f3b70dc92cf1446cbc",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38bd2d0bf2744b2c98560a4c18cc7c5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=938), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988f335c755c4e48af8f01b2f6b61cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=247), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Get stopword count\n",
      "Convert to lower\n",
      "Clean the text\n",
      "Convert the spellings\n",
      "Get character count\n",
      "Get subjectivity\n",
      "Get word density\n",
      "Get punctuation count\n",
      "Get polarity\n",
      "Get subjectivity\n",
      "Get stopword count\n",
      "Convert to lower\n",
      "Clean the text\n",
      "Convert the spellings\n",
      "Get character count\n",
      "Get subjectivity\n",
      "Get word density\n",
      "Get punctuation count\n",
      "Get polarity\n",
      "Get subjectivity\n"
     ]
    }
   ],
   "source": [
    "cols_to_remove = ['Name', 'RescuerID', 'Description', 'PetID']\n",
    "\n",
    "train, test = train_orig.copy(), test_orig.copy()\n",
    "\n",
    "# # Get text area in image feature\n",
    "# ia = image_annotation_features()\n",
    "# ia.annotation_fies = [gv_rel_text_area_feature]\n",
    "# ia.dataclass = \"test\"\n",
    "# ia.get_ftr(test)\n",
    "# ia.dataclass = \"train\"\n",
    "# feats = ia.get_ftr(train)\n",
    "\n",
    "# Get rescuer experience feature\n",
    "get_rescuer_experience(train, test)\n",
    "\n",
    "# Get TFIDF and SVD features\n",
    "train, test = get_svd_features(train, test)\n",
    "\n",
    "# Get description sentiment from JSON data\n",
    "train = get_description_sentiment(train, \"train\")\n",
    "test = get_description_sentiment(test, \"test\")\n",
    "\n",
    "# Get image metadata\n",
    "train = get_image_metadata(train, \"train\")\n",
    "test = get_image_metadata(test, \"test\")\n",
    "\n",
    "# Create some custom features\n",
    "train = get_aggregate_features(train)\n",
    "test = get_aggregate_features(test)\n",
    "\n",
    "# Get raw image features\n",
    "train = get_raw_image_features(train, \"train\")\n",
    "test = get_raw_image_features(test, \"test\")\n",
    "\n",
    "# Get textual features\n",
    "train = get_textual_features_from_description(train)\n",
    "test = get_textual_features_from_description(test)\n",
    "\n",
    "# Get state features\n",
    "train = get_state_features(train)\n",
    "test = get_state_features(test)\n",
    "\n",
    "# Get target\n",
    "target = train['AdoptionSpeed']\n",
    "train = train.drop('AdoptionSpeed', 1)\n",
    "\n",
    "# Save RescuerID for fold grouping\n",
    "rescuerid = train.RescuerID\n",
    "\n",
    "# Remove unnecessary columns\n",
    "train = train.drop(cols_to_remove, 1)\n",
    "test = test.drop(cols_to_remove, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "e23a34fcccc5f3b582cbe138b17be6af50b33c50"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14993, 565)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "6176a766cbd3ba655776136571c63a0c172d159f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([                         'Type',                           'Age',\n",
       "                              'Breed1',                        'Breed2',\n",
       "                              'Gender',                        'Color1',\n",
       "                              'Color2',                        'Color3',\n",
       "                        'MaturitySize',                     'FurLength',\n",
       "       ...\n",
       "          'Description_stopword_count',   'Description_character_count',\n",
       "              'Description_word_count',      'Description_word_density',\n",
       "       'Description_punctuation_count',          'Description_polarity',\n",
       "            'Description_subjectivity',                     'state_gdp',\n",
       "                    'state_population',                    'state_area'],\n",
       "      dtype='object', length=565)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "360fe0003e3b059bfdf1d760975aa3b55482d514",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Type</th>\n",
       "      <th>Age</th>\n",
       "      <th>Breed1</th>\n",
       "      <th>Breed2</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Color1</th>\n",
       "      <th>Color2</th>\n",
       "      <th>Color3</th>\n",
       "      <th>MaturitySize</th>\n",
       "      <th>FurLength</th>\n",
       "      <th>Vaccinated</th>\n",
       "      <th>Dewormed</th>\n",
       "      <th>Sterilized</th>\n",
       "      <th>Health</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Fee</th>\n",
       "      <th>State</th>\n",
       "      <th>VideoAmt</th>\n",
       "      <th>PhotoAmt</th>\n",
       "      <th>rescuer_1_1</th>\n",
       "      <th>rescuer_1_3</th>\n",
       "      <th>rescuer_1_2</th>\n",
       "      <th>rescuer_1_4</th>\n",
       "      <th>rescuer_2_1</th>\n",
       "      <th>rescuer_2_3</th>\n",
       "      <th>...</th>\n",
       "      <th>241</th>\n",
       "      <th>242</th>\n",
       "      <th>243</th>\n",
       "      <th>244</th>\n",
       "      <th>245</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>Description_stopword_count</th>\n",
       "      <th>Description_character_count</th>\n",
       "      <th>Description_word_count</th>\n",
       "      <th>Description_word_density</th>\n",
       "      <th>Description_punctuation_count</th>\n",
       "      <th>Description_polarity</th>\n",
       "      <th>Description_subjectivity</th>\n",
       "      <th>state_gdp</th>\n",
       "      <th>state_population</th>\n",
       "      <th>state_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>299</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>41326</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.337702</td>\n",
       "      <td>0.150102</td>\n",
       "      <td>1.081104</td>\n",
       "      <td>1.282496</td>\n",
       "      <td>0.643566</td>\n",
       "      <td>0.787699</td>\n",
       "      <td>0.176625</td>\n",
       "      <td>0.575706</td>\n",
       "      <td>1.088628</td>\n",
       "      <td>0.439556</td>\n",
       "      <td>0.520460</td>\n",
       "      <td>1.547070</td>\n",
       "      <td>0.832572</td>\n",
       "      <td>0.599093</td>\n",
       "      <td>0.763348</td>\n",
       "      <td>34</td>\n",
       "      <td>501</td>\n",
       "      <td>167</td>\n",
       "      <td>2.982143</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>280.698</td>\n",
       "      <td>54.62141</td>\n",
       "      <td>8104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>265</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41401</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.944075</td>\n",
       "      <td>0.481491</td>\n",
       "      <td>0.977006</td>\n",
       "      <td>1.295760</td>\n",
       "      <td>0.751895</td>\n",
       "      <td>0.628259</td>\n",
       "      <td>0.686865</td>\n",
       "      <td>0.563999</td>\n",
       "      <td>0.968191</td>\n",
       "      <td>1.070276</td>\n",
       "      <td>1.545742</td>\n",
       "      <td>0.894409</td>\n",
       "      <td>0.838596</td>\n",
       "      <td>0.468237</td>\n",
       "      <td>0.916672</td>\n",
       "      <td>12</td>\n",
       "      <td>137</td>\n",
       "      <td>43</td>\n",
       "      <td>3.113636</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.075</td>\n",
       "      <td>16.74621</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41326</td>\n",
       "      <td>0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>399.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.705539</td>\n",
       "      <td>0.793940</td>\n",
       "      <td>0.469396</td>\n",
       "      <td>0.278331</td>\n",
       "      <td>1.043843</td>\n",
       "      <td>0.579116</td>\n",
       "      <td>0.557624</td>\n",
       "      <td>1.131406</td>\n",
       "      <td>0.720514</td>\n",
       "      <td>1.496670</td>\n",
       "      <td>0.870955</td>\n",
       "      <td>1.289683</td>\n",
       "      <td>1.184462</td>\n",
       "      <td>0.465114</td>\n",
       "      <td>0.892826</td>\n",
       "      <td>30</td>\n",
       "      <td>513</td>\n",
       "      <td>171</td>\n",
       "      <td>2.982558</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>280.698</td>\n",
       "      <td>54.62141</td>\n",
       "      <td>8104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>150</td>\n",
       "      <td>41401</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.690941</td>\n",
       "      <td>1.246175</td>\n",
       "      <td>0.743014</td>\n",
       "      <td>1.178628</td>\n",
       "      <td>0.865275</td>\n",
       "      <td>1.295853</td>\n",
       "      <td>0.326143</td>\n",
       "      <td>0.291669</td>\n",
       "      <td>1.608087</td>\n",
       "      <td>1.119176</td>\n",
       "      <td>1.470889</td>\n",
       "      <td>0.591444</td>\n",
       "      <td>0.832755</td>\n",
       "      <td>0.483021</td>\n",
       "      <td>1.134127</td>\n",
       "      <td>8</td>\n",
       "      <td>209</td>\n",
       "      <td>67</td>\n",
       "      <td>3.073529</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>190.075</td>\n",
       "      <td>16.74621</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>307</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>41326</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.418224</td>\n",
       "      <td>0.187639</td>\n",
       "      <td>1.176707</td>\n",
       "      <td>0.638147</td>\n",
       "      <td>0.425842</td>\n",
       "      <td>1.092663</td>\n",
       "      <td>0.669894</td>\n",
       "      <td>0.395784</td>\n",
       "      <td>0.886075</td>\n",
       "      <td>1.219729</td>\n",
       "      <td>1.033965</td>\n",
       "      <td>1.065686</td>\n",
       "      <td>0.304054</td>\n",
       "      <td>0.438069</td>\n",
       "      <td>0.676818</td>\n",
       "      <td>42</td>\n",
       "      <td>559</td>\n",
       "      <td>182</td>\n",
       "      <td>3.054645</td>\n",
       "      <td>9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>280.698</td>\n",
       "      <td>54.62141</td>\n",
       "      <td>8104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 565 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Type  Age  Breed1  Breed2  Gender  Color1  Color2  Color3  MaturitySize  \\\n",
       "0     2    3     299       0       1       1       7       0             1   \n",
       "1     2    1     265       0       1       1       2       0             2   \n",
       "2     1    1     307       0       1       2       7       0             2   \n",
       "3     1    4     307       0       2       1       2       0             2   \n",
       "4     1    1     307       0       1       1       0       0             2   \n",
       "\n",
       "   FurLength  Vaccinated  Dewormed  Sterilized  Health  Quantity  Fee  State  \\\n",
       "0          1           2         2           2       1         1  100  41326   \n",
       "1          2           3         3           3       1         1    0  41401   \n",
       "2          2           1         1           2       1         1    0  41326   \n",
       "3          1           1         1           2       1         1  150  41401   \n",
       "4          1           2         2           2       1         1    0  41326   \n",
       "\n",
       "   VideoAmt  PhotoAmt  rescuer_1_1  rescuer_1_3  rescuer_1_2  rescuer_1_4  \\\n",
       "0         0       1.0          0.0          0.0          0.0          0.0   \n",
       "1         0       2.0          0.0          0.0          0.0          0.0   \n",
       "2         0       7.0          6.0         44.0        399.0          6.0   \n",
       "3         0       8.0          0.0          1.0         40.0          0.0   \n",
       "4         0       3.0          8.0          4.0        118.0          0.0   \n",
       "\n",
       "   rescuer_2_1  rescuer_2_3     ...           241       242       243  \\\n",
       "0          2.0          0.0     ...      1.337702  0.150102  1.081104   \n",
       "1          0.0          0.0     ...      1.944075  0.481491  0.977006   \n",
       "2          0.0          0.0     ...      0.705539  0.793940  0.469396   \n",
       "3          1.0          0.0     ...      1.690941  1.246175  0.743014   \n",
       "4          1.0          0.0     ...      1.418224  0.187639  1.176707   \n",
       "\n",
       "        244       245       246       247       248       249       250  \\\n",
       "0  1.282496  0.643566  0.787699  0.176625  0.575706  1.088628  0.439556   \n",
       "1  1.295760  0.751895  0.628259  0.686865  0.563999  0.968191  1.070276   \n",
       "2  0.278331  1.043843  0.579116  0.557624  1.131406  0.720514  1.496670   \n",
       "3  1.178628  0.865275  1.295853  0.326143  0.291669  1.608087  1.119176   \n",
       "4  0.638147  0.425842  1.092663  0.669894  0.395784  0.886075  1.219729   \n",
       "\n",
       "        251       252       253       254       255  \\\n",
       "0  0.520460  1.547070  0.832572  0.599093  0.763348   \n",
       "1  1.545742  0.894409  0.838596  0.468237  0.916672   \n",
       "2  0.870955  1.289683  1.184462  0.465114  0.892826   \n",
       "3  1.470889  0.591444  0.832755  0.483021  1.134127   \n",
       "4  1.033965  1.065686  0.304054  0.438069  0.676818   \n",
       "\n",
       "   Description_stopword_count  Description_character_count  \\\n",
       "0                          34                          501   \n",
       "1                          12                          137   \n",
       "2                          30                          513   \n",
       "3                           8                          209   \n",
       "4                          42                          559   \n",
       "\n",
       "   Description_word_count  Description_word_density  \\\n",
       "0                     167                  2.982143   \n",
       "1                      43                  3.113636   \n",
       "2                     171                  2.982558   \n",
       "3                      67                  3.073529   \n",
       "4                     182                  3.054645   \n",
       "\n",
       "   Description_punctuation_count  Description_polarity  \\\n",
       "0                              8                   0.0   \n",
       "1                              2                   0.0   \n",
       "2                              9                   0.0   \n",
       "3                              7                   0.0   \n",
       "4                              9                   0.0   \n",
       "\n",
       "   Description_subjectivity  state_gdp  state_population  state_area  \n",
       "0                       0.0    280.698          54.62141        8104  \n",
       "1                       0.0    190.075          16.74621         243  \n",
       "2                       0.0    280.698          54.62141        8104  \n",
       "3                       0.0    190.075          16.74621         243  \n",
       "4                       0.0    280.698          54.62141        8104  \n",
       "\n",
       "[5 rows x 565 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fb7fb3ba0f106e16217ae44b059f7681021e7bbd"
   },
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "585a6d7777315b2fae71837a07484b8ef297b8e2"
   },
   "outputs": [],
   "source": [
    "# Metric used for this competition (Quadratic Weigthed Kappa aka Quadratic Cohen Kappa Score)\n",
    "def metric_function(y1, y2):\n",
    "    return cohen_kappa_score(y1, y2, weights = 'quadratic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "2244d7b84e45aaec87afd2d8833374c5439aef1b"
   },
   "outputs": [],
   "source": [
    "def get_class_bounds(y, y_pred, N = 5, class0_fraction = -1):\n",
    "    ysort = np.sort(y)\n",
    "    predsort = np.sort(y_pred)\n",
    "    bounds = []\n",
    "    for ibound in range(N-1):\n",
    "        iy = len(ysort[ysort <= ibound])\n",
    "        # adjust the number of class 0 predictions?\n",
    "        if (ibound == 0) and (class0_fraction >= 0.0) :\n",
    "            iy = int(class0_fraction * iy)\n",
    "        bounds.append(predsort[iy])\n",
    "    return bounds\n",
    "\n",
    "def assign_class(y_pred, boundaries):\n",
    "    \"\"\"\n",
    "    Given class boundaries in y_pred units, output integer class values\n",
    "    \"\"\"\n",
    "    y_classes = np.zeros(len(y_pred))\n",
    "    for iclass, bound in enumerate(boundaries):\n",
    "        y_classes[y_pred >= bound] = iclass + 1\n",
    "    return y_classes.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "22d273ddad6376a65b2a3570e1fa9561fd9ebf50"
   },
   "outputs": [],
   "source": [
    "# put some numerical values to bins\n",
    "def to_bins(x, borders):\n",
    "    for i in range(len(borders)):\n",
    "        if x <= borders[i]:\n",
    "            return i\n",
    "    return len(borders)\n",
    "\n",
    "class OptimizedRounder(object):\n",
    "    def __init__(self):\n",
    "        self.coef_ = 0\n",
    "\n",
    "    def _loss(self, coef, X, y, idx):\n",
    "        X_p = np.array([to_bins(pred, coef) for pred in X])\n",
    "        ll = -metric_function(y, X_p)\n",
    "        return ll\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        coef = [1.5, 2.0, 2.5, 3.0]\n",
    "        golden1 = 0.618\n",
    "        golden2 = 1 - golden1\n",
    "        ab_start = [(1, 2), (1.5, 2.5), (2, 3), (2.5, 3.5)]\n",
    "        for it1 in range(10):\n",
    "            for idx in range(4):\n",
    "                # golden section search\n",
    "                a, b = ab_start[idx]\n",
    "                # calc losses\n",
    "                coef[idx] = a\n",
    "                la = self._loss(coef, X, y, idx)\n",
    "                coef[idx] = b\n",
    "                lb = self._loss(coef, X, y, idx)\n",
    "                for it in range(20):\n",
    "                    # choose value\n",
    "                    if la > lb:\n",
    "                        a = b - (b - a) * golden1\n",
    "                        coef[idx] = a\n",
    "                        la = self._loss(coef, X, y, idx)\n",
    "                    else:\n",
    "                        b = b - (b - a) * golden2\n",
    "                        coef[idx] = b\n",
    "                        lb = self._loss(coef, X, y, idx)\n",
    "        self.coef_ = {'x': coef}\n",
    "\n",
    "    def predict(self, X, coef):\n",
    "        X_p = np.array([to_bins(pred, coef) for pred in X])\n",
    "        return X_p\n",
    "\n",
    "    def coefficients(self):\n",
    "        return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "c63d3a18e54f1f6f4a6d43078a4b008ca28e3823"
   },
   "outputs": [],
   "source": [
    "# class OptimizedRounder(object):\n",
    "#     def __init__(self):\n",
    "#         self.coef_ = 0\n",
    "\n",
    "#     def _kappa_loss(self, coef, X, y):\n",
    "#         X_p = np.copy(X)\n",
    "#         for i, pred in enumerate(X_p):\n",
    "#             if pred < coef[0]:\n",
    "#                 X_p[i] = 0\n",
    "#             elif pred >= coef[0] and pred < coef[1]:\n",
    "#                 X_p[i] = 1\n",
    "#             elif pred >= coef[1] and pred < coef[2]:\n",
    "#                 X_p[i] = 2\n",
    "#             elif pred >= coef[2] and pred < coef[3]:\n",
    "#                 X_p[i] = 3\n",
    "#             else:\n",
    "#                 X_p[i] = 4\n",
    "\n",
    "#         ll = metric_function(y, X_p)\n",
    "#         return -ll\n",
    "\n",
    "#     def fit(self, X, y):\n",
    "#         loss_partial = partial(self._kappa_loss, X = X, y = y)\n",
    "        \n",
    "#         cl0fracs = np.array(np.arange(0.01, 1.001, 0.01))\n",
    "#         boundaries = []\n",
    "#         kappas = []\n",
    "#         for cl0frac in cl0fracs:\n",
    "#             boundary = get_class_bounds(y, X, class0_fraction = cl0frac)\n",
    "#             train_meta_ints = assign_class(X, boundary)\n",
    "#             kappa = metric_function(y, train_meta_ints)\n",
    "#             kappas.append(kappa)\n",
    "#             boundaries.append(boundary)\n",
    "#         max_kappa_index = np.array(kappas).argmax()\n",
    "#         initial_coef = boundaries[max_kappa_index]\n",
    "        \n",
    "#         self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method = 'nelder-mead')\n",
    "\n",
    "#     def predict(self, X, coef):\n",
    "#         X_p = np.copy(X)\n",
    "#         for i, pred in enumerate(X_p):\n",
    "#             if pred < coef[0]:\n",
    "#                 X_p[i] = 0\n",
    "#             elif pred >= coef[0] and pred < coef[1]:\n",
    "#                 X_p[i] = 1\n",
    "#             elif pred >= coef[1] and pred < coef[2]:\n",
    "#                 X_p[i] = 2\n",
    "#             elif pred >= coef[2] and pred < coef[3]:\n",
    "#                 X_p[i] = 3\n",
    "#             else:\n",
    "#                 X_p[i] = 4\n",
    "#         return X_p\n",
    "\n",
    "#     def coefficients(self):\n",
    "#         return self.coef_['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "b0aa90db89b45748670560d4d94a9e69f7cdff1e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l2: 1.14484\tvalid_0's rmse: 1.06997\n",
      "[200]\tvalid_0's l2: 1.08875\tvalid_0's rmse: 1.04343\n",
      "[300]\tvalid_0's l2: 1.06215\tvalid_0's rmse: 1.03061\n",
      "[400]\tvalid_0's l2: 1.05039\tvalid_0's rmse: 1.02488\n",
      "[500]\tvalid_0's l2: 1.04462\tvalid_0's rmse: 1.02207\n",
      "[600]\tvalid_0's l2: 1.04157\tvalid_0's rmse: 1.02057\n",
      "[700]\tvalid_0's l2: 1.04059\tvalid_0's rmse: 1.02009\n",
      "[800]\tvalid_0's l2: 1.03841\tvalid_0's rmse: 1.01903\n",
      "[900]\tvalid_0's l2: 1.0377\tvalid_0's rmse: 1.01868\n",
      "Early stopping, best iteration is:\n",
      "[857]\tvalid_0's l2: 1.03703\tvalid_0's rmse: 1.01834\n",
      "Fold = 1. QWK = 0.4300947097947181.\n",
      "\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l2: 1.21137\tvalid_0's rmse: 1.10062\n",
      "[200]\tvalid_0's l2: 1.14175\tvalid_0's rmse: 1.06853\n",
      "[300]\tvalid_0's l2: 1.10642\tvalid_0's rmse: 1.05186\n",
      "[400]\tvalid_0's l2: 1.08683\tvalid_0's rmse: 1.04251\n",
      "[500]\tvalid_0's l2: 1.0765\tvalid_0's rmse: 1.03755\n",
      "[600]\tvalid_0's l2: 1.06931\tvalid_0's rmse: 1.03407\n",
      "[700]\tvalid_0's l2: 1.06513\tvalid_0's rmse: 1.03205\n",
      "[800]\tvalid_0's l2: 1.06325\tvalid_0's rmse: 1.03114\n",
      "[900]\tvalid_0's l2: 1.06146\tvalid_0's rmse: 1.03027\n",
      "[1000]\tvalid_0's l2: 1.06034\tvalid_0's rmse: 1.02973\n",
      "[1100]\tvalid_0's l2: 1.05897\tvalid_0's rmse: 1.02906\n",
      "[1200]\tvalid_0's l2: 1.05803\tvalid_0's rmse: 1.0286\n",
      "[1300]\tvalid_0's l2: 1.05735\tvalid_0's rmse: 1.02827\n",
      "[1400]\tvalid_0's l2: 1.05769\tvalid_0's rmse: 1.02844\n",
      "Early stopping, best iteration is:\n",
      "[1344]\tvalid_0's l2: 1.05726\tvalid_0's rmse: 1.02823\n",
      "Fold = 2. QWK = 0.46309298176602176.\n",
      "\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l2: 1.1781\tvalid_0's rmse: 1.0854\n",
      "[200]\tvalid_0's l2: 1.12663\tvalid_0's rmse: 1.06143\n",
      "[300]\tvalid_0's l2: 1.10082\tvalid_0's rmse: 1.0492\n",
      "[400]\tvalid_0's l2: 1.08884\tvalid_0's rmse: 1.04348\n",
      "[500]\tvalid_0's l2: 1.08259\tvalid_0's rmse: 1.04048\n",
      "[600]\tvalid_0's l2: 1.07835\tvalid_0's rmse: 1.03844\n",
      "[700]\tvalid_0's l2: 1.07441\tvalid_0's rmse: 1.03654\n",
      "[800]\tvalid_0's l2: 1.07263\tvalid_0's rmse: 1.03568\n",
      "[900]\tvalid_0's l2: 1.07079\tvalid_0's rmse: 1.03479\n",
      "[1000]\tvalid_0's l2: 1.07157\tvalid_0's rmse: 1.03516\n",
      "Early stopping, best iteration is:\n",
      "[917]\tvalid_0's l2: 1.07051\tvalid_0's rmse: 1.03465\n",
      "Fold = 3. QWK = 0.42479811311913274.\n",
      "\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l2: 1.26227\tvalid_0's rmse: 1.12351\n",
      "[200]\tvalid_0's l2: 1.20589\tvalid_0's rmse: 1.09813\n",
      "[300]\tvalid_0's l2: 1.17633\tvalid_0's rmse: 1.08459\n",
      "[400]\tvalid_0's l2: 1.16183\tvalid_0's rmse: 1.07788\n",
      "[500]\tvalid_0's l2: 1.15297\tvalid_0's rmse: 1.07377\n",
      "[600]\tvalid_0's l2: 1.14833\tvalid_0's rmse: 1.0716\n",
      "[700]\tvalid_0's l2: 1.14493\tvalid_0's rmse: 1.07001\n",
      "[800]\tvalid_0's l2: 1.14177\tvalid_0's rmse: 1.06854\n",
      "[900]\tvalid_0's l2: 1.14072\tvalid_0's rmse: 1.06805\n",
      "[1000]\tvalid_0's l2: 1.1392\tvalid_0's rmse: 1.06733\n",
      "[1100]\tvalid_0's l2: 1.13759\tvalid_0's rmse: 1.06658\n",
      "[1200]\tvalid_0's l2: 1.13643\tvalid_0's rmse: 1.06604\n",
      "[1300]\tvalid_0's l2: 1.13472\tvalid_0's rmse: 1.06523\n",
      "[1400]\tvalid_0's l2: 1.13414\tvalid_0's rmse: 1.06496\n",
      "[1500]\tvalid_0's l2: 1.13304\tvalid_0's rmse: 1.06444\n",
      "[1600]\tvalid_0's l2: 1.13258\tvalid_0's rmse: 1.06423\n",
      "Early stopping, best iteration is:\n",
      "[1546]\tvalid_0's l2: 1.1322\tvalid_0's rmse: 1.06405\n",
      "Fold = 4. QWK = 0.42259756059866493.\n",
      "\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l2: 1.24809\tvalid_0's rmse: 1.11718\n",
      "[200]\tvalid_0's l2: 1.18284\tvalid_0's rmse: 1.08758\n",
      "[300]\tvalid_0's l2: 1.15197\tvalid_0's rmse: 1.0733\n",
      "[400]\tvalid_0's l2: 1.13483\tvalid_0's rmse: 1.06529\n",
      "[500]\tvalid_0's l2: 1.1265\tvalid_0's rmse: 1.06137\n",
      "[600]\tvalid_0's l2: 1.12219\tvalid_0's rmse: 1.05934\n",
      "[700]\tvalid_0's l2: 1.11738\tvalid_0's rmse: 1.05706\n",
      "[800]\tvalid_0's l2: 1.11398\tvalid_0's rmse: 1.05545\n",
      "[900]\tvalid_0's l2: 1.11189\tvalid_0's rmse: 1.05446\n",
      "[1000]\tvalid_0's l2: 1.1098\tvalid_0's rmse: 1.05347\n",
      "[1100]\tvalid_0's l2: 1.10815\tvalid_0's rmse: 1.05269\n",
      "[1200]\tvalid_0's l2: 1.10756\tvalid_0's rmse: 1.05241\n",
      "[1300]\tvalid_0's l2: 1.10649\tvalid_0's rmse: 1.0519\n",
      "[1400]\tvalid_0's l2: 1.10599\tvalid_0's rmse: 1.05166\n",
      "[1500]\tvalid_0's l2: 1.10552\tvalid_0's rmse: 1.05144\n",
      "[1600]\tvalid_0's l2: 1.10619\tvalid_0's rmse: 1.05176\n",
      "Early stopping, best iteration is:\n",
      "[1502]\tvalid_0's l2: 1.1054\tvalid_0's rmse: 1.05138\n",
      "Fold = 5. QWK = 0.45185565159666286.\n",
      "\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l2: 1.23247\tvalid_0's rmse: 1.11017\n",
      "[200]\tvalid_0's l2: 1.18691\tvalid_0's rmse: 1.08945\n",
      "[300]\tvalid_0's l2: 1.16652\tvalid_0's rmse: 1.08005\n",
      "[400]\tvalid_0's l2: 1.15961\tvalid_0's rmse: 1.07685\n",
      "[500]\tvalid_0's l2: 1.15569\tvalid_0's rmse: 1.07503\n",
      "[600]\tvalid_0's l2: 1.1516\tvalid_0's rmse: 1.07312\n",
      "[700]\tvalid_0's l2: 1.14996\tvalid_0's rmse: 1.07236\n",
      "[800]\tvalid_0's l2: 1.14885\tvalid_0's rmse: 1.07185\n",
      "[900]\tvalid_0's l2: 1.14786\tvalid_0's rmse: 1.07138\n",
      "[1000]\tvalid_0's l2: 1.1457\tvalid_0's rmse: 1.07037\n",
      "[1100]\tvalid_0's l2: 1.14513\tvalid_0's rmse: 1.07011\n",
      "[1200]\tvalid_0's l2: 1.14466\tvalid_0's rmse: 1.06989\n",
      "[1300]\tvalid_0's l2: 1.14418\tvalid_0's rmse: 1.06966\n",
      "[1400]\tvalid_0's l2: 1.14485\tvalid_0's rmse: 1.06998\n",
      "Early stopping, best iteration is:\n",
      "[1346]\tvalid_0's l2: 1.144\tvalid_0's rmse: 1.06958\n",
      "Fold = 6. QWK = 0.3894511938575993.\n",
      "\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l2: 1.31297\tvalid_0's rmse: 1.14585\n",
      "[200]\tvalid_0's l2: 1.23796\tvalid_0's rmse: 1.11264\n",
      "[300]\tvalid_0's l2: 1.19961\tvalid_0's rmse: 1.09527\n",
      "[400]\tvalid_0's l2: 1.17813\tvalid_0's rmse: 1.08542\n",
      "[500]\tvalid_0's l2: 1.16615\tvalid_0's rmse: 1.07988\n",
      "[600]\tvalid_0's l2: 1.15691\tvalid_0's rmse: 1.0756\n",
      "[700]\tvalid_0's l2: 1.15107\tvalid_0's rmse: 1.07288\n",
      "[800]\tvalid_0's l2: 1.14639\tvalid_0's rmse: 1.0707\n",
      "[900]\tvalid_0's l2: 1.14403\tvalid_0's rmse: 1.06959\n",
      "[1000]\tvalid_0's l2: 1.14148\tvalid_0's rmse: 1.0684\n",
      "[1100]\tvalid_0's l2: 1.14132\tvalid_0's rmse: 1.06833\n",
      "[1200]\tvalid_0's l2: 1.14061\tvalid_0's rmse: 1.06799\n",
      "[1300]\tvalid_0's l2: 1.14063\tvalid_0's rmse: 1.068\n",
      "[1400]\tvalid_0's l2: 1.14013\tvalid_0's rmse: 1.06777\n",
      "Early stopping, best iteration is:\n",
      "[1380]\tvalid_0's l2: 1.13945\tvalid_0's rmse: 1.06745\n",
      "Fold = 7. QWK = 0.4801881374776149.\n",
      "\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l2: 1.2651\tvalid_0's rmse: 1.12476\n",
      "[200]\tvalid_0's l2: 1.21286\tvalid_0's rmse: 1.1013\n",
      "[300]\tvalid_0's l2: 1.18715\tvalid_0's rmse: 1.08956\n",
      "[400]\tvalid_0's l2: 1.17474\tvalid_0's rmse: 1.08386\n",
      "[500]\tvalid_0's l2: 1.16608\tvalid_0's rmse: 1.07985\n",
      "[600]\tvalid_0's l2: 1.16036\tvalid_0's rmse: 1.0772\n",
      "[700]\tvalid_0's l2: 1.1567\tvalid_0's rmse: 1.0755\n",
      "[800]\tvalid_0's l2: 1.15313\tvalid_0's rmse: 1.07384\n",
      "[900]\tvalid_0's l2: 1.15186\tvalid_0's rmse: 1.07325\n",
      "[1000]\tvalid_0's l2: 1.1513\tvalid_0's rmse: 1.07299\n",
      "[1100]\tvalid_0's l2: 1.14995\tvalid_0's rmse: 1.07235\n",
      "[1200]\tvalid_0's l2: 1.14994\tvalid_0's rmse: 1.07235\n",
      "[1300]\tvalid_0's l2: 1.14886\tvalid_0's rmse: 1.07185\n",
      "[1400]\tvalid_0's l2: 1.14817\tvalid_0's rmse: 1.07153\n",
      "[1500]\tvalid_0's l2: 1.14838\tvalid_0's rmse: 1.07163\n",
      "Early stopping, best iteration is:\n",
      "[1462]\tvalid_0's l2: 1.14798\tvalid_0's rmse: 1.07144\n",
      "Fold = 8. QWK = 0.4064936771453963.\n",
      "\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l2: 1.31494\tvalid_0's rmse: 1.14671\n",
      "[200]\tvalid_0's l2: 1.22389\tvalid_0's rmse: 1.1063\n",
      "[300]\tvalid_0's l2: 1.1763\tvalid_0's rmse: 1.08457\n",
      "[400]\tvalid_0's l2: 1.15015\tvalid_0's rmse: 1.07245\n",
      "[500]\tvalid_0's l2: 1.13585\tvalid_0's rmse: 1.06576\n",
      "[600]\tvalid_0's l2: 1.12544\tvalid_0's rmse: 1.06087\n",
      "[700]\tvalid_0's l2: 1.1195\tvalid_0's rmse: 1.05807\n",
      "[800]\tvalid_0's l2: 1.11492\tvalid_0's rmse: 1.0559\n",
      "[900]\tvalid_0's l2: 1.11267\tvalid_0's rmse: 1.05483\n",
      "[1000]\tvalid_0's l2: 1.11054\tvalid_0's rmse: 1.05382\n",
      "[1100]\tvalid_0's l2: 1.10986\tvalid_0's rmse: 1.0535\n",
      "[1200]\tvalid_0's l2: 1.10792\tvalid_0's rmse: 1.05258\n",
      "[1300]\tvalid_0's l2: 1.10695\tvalid_0's rmse: 1.05212\n",
      "[1400]\tvalid_0's l2: 1.10625\tvalid_0's rmse: 1.05178\n",
      "[1500]\tvalid_0's l2: 1.10679\tvalid_0's rmse: 1.05204\n",
      "Early stopping, best iteration is:\n",
      "[1415]\tvalid_0's l2: 1.10604\tvalid_0's rmse: 1.05169\n",
      "Fold = 9. QWK = 0.5278608247715741.\n",
      "\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds.\n",
      "[100]\tvalid_0's l2: 1.26713\tvalid_0's rmse: 1.12567\n",
      "[200]\tvalid_0's l2: 1.21149\tvalid_0's rmse: 1.10068\n",
      "[300]\tvalid_0's l2: 1.1882\tvalid_0's rmse: 1.09005\n",
      "[400]\tvalid_0's l2: 1.17694\tvalid_0's rmse: 1.08487\n",
      "[500]\tvalid_0's l2: 1.17231\tvalid_0's rmse: 1.08273\n",
      "[600]\tvalid_0's l2: 1.16653\tvalid_0's rmse: 1.08006\n",
      "[700]\tvalid_0's l2: 1.16324\tvalid_0's rmse: 1.07854\n",
      "[800]\tvalid_0's l2: 1.16176\tvalid_0's rmse: 1.07785\n",
      "[900]\tvalid_0's l2: 1.16068\tvalid_0's rmse: 1.07735\n",
      "[1000]\tvalid_0's l2: 1.16027\tvalid_0's rmse: 1.07716\n",
      "[1100]\tvalid_0's l2: 1.16002\tvalid_0's rmse: 1.07704\n",
      "Early stopping, best iteration is:\n",
      "[1032]\tvalid_0's l2: 1.15921\tvalid_0's rmse: 1.07666\n",
      "Fold = 10. QWK = 0.406369673785533.\n",
      "\n",
      "\n",
      "Mean Score: 0.44028025239129176. Std Dev: 0.03930468056341005. Mean Coeff: [1.38216078 2.13020673 2.5386525  2.88142121]\n"
     ]
    }
   ],
   "source": [
    "FOLDS = 10\n",
    "train_predictions = np.zeros((train.shape[0], 1))\n",
    "test_predictions = np.zeros((test.shape[0], 1))\n",
    "zero_test_predictions = np.zeros((test.shape[0], 1))\n",
    "\n",
    "# print(\"stratified k-folds\")\n",
    "# cv = StratifiedKFold(n_splits = FOLDS, random_state = 42, shuffle = False)\n",
    "# cv.get_n_splits(train, target)\n",
    "\n",
    "# print(\"stratified grouped k-folds\")\n",
    "cv = GroupKFold(n_splits = FOLDS)\n",
    "cv.get_n_splits(train, target, rescuerid)\n",
    "\n",
    "cv_scores = []\n",
    "fold = 1\n",
    "coefficients = np.zeros((FOLDS, 4))\n",
    "for train_idx, valid_idx in cv.split(train, target, rescuerid):\n",
    "    xtrain, xvalid = train.iloc[train_idx], train.iloc[valid_idx]\n",
    "    ytrain, yvalid = target.iloc[train_idx], target.iloc[valid_idx]\n",
    "    \n",
    "    lgb_params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'regression',\n",
    "            'learning_rate': 0.006,\n",
    "            'subsample': .8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'n_estimators': 10000,\n",
    "            'silent': -1,\n",
    "            'verbose': -1,\n",
    "            'random_state': 420,\n",
    "            'bagging_fraction': 0.9212945843023237,\n",
    "            'bagging_freq': int(2.1100859370529492),\n",
    "            'feature_fraction': 0.6334740217238963,\n",
    "            'lambda_l2': 1.543309192604612,\n",
    "            'max_bin': int(32.46977068537903),\n",
    "            'max_depth': int(11.982021953762485),\n",
    "            'min_child_samples': int(44.96596724925662),\n",
    "            'min_child_weight': 0.5878240657385082,\n",
    "            'min_split_gain': 0.004619759404679957,\n",
    "            'num_leaves': int(146.73598418222304)\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMRegressor(**lgb_params)\n",
    "    model.fit(\n",
    "        xtrain, ytrain,\n",
    "        eval_set = [(xvalid, yvalid)],\n",
    "        eval_metric = 'rmse',\n",
    "        verbose = 100,\n",
    "        early_stopping_rounds = 100\n",
    "    )\n",
    "    \n",
    "    valid_preds = model.predict(xvalid, num_iteration = model.best_iteration_)\n",
    "    optR = OptimizedRounder()\n",
    "    optR.fit(valid_preds, yvalid.values)\n",
    "    coefficients[fold - 1, :] = optR.coefficients()\n",
    "    valid_p = optR.predict(valid_preds, coefficients[fold-1,:])\n",
    "    scr = metric_function(yvalid.values, valid_p)\n",
    "    cv_scores.append(scr)\n",
    "    print(\"Fold = {}. QWK = {}.\".format(fold, scr))\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    test_preds = model.predict(test, num_iteration = model.best_iteration_)\n",
    "    train_predictions[valid_idx] = valid_preds.reshape(-1, 1)\n",
    "    test_predictions += test_preds.reshape(-1, 1)\n",
    "    fold += 1\n",
    "test_predictions = test_predictions * 1./FOLDS\n",
    "print(\"Mean Score: {}. Std Dev: {}. Mean Coeff: {}\".format(np.mean(cv_scores), np.std(cv_scores), np.mean(coefficients, axis = 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optR = OptimizedRounder()\n",
    "optR.fit(train_predictions.reshape(1, -1)[0], target.values)\n",
    "coefficients = optR.coefficients()\n",
    "predictions = optR.predict(test_predictions, coefficients).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "088d74226313f378d1f6493f2530d3bcc7a2678a"
   },
   "outputs": [],
   "source": [
    "# optR = OptimizedRounder()\n",
    "# optR.fit(train_predictions.reshape(1,-1)[0], target.values)\n",
    "# coefficients = optR.coefficients()\n",
    "\n",
    "# # Manually adjust coefficients\n",
    "# coefficients_ = coefficients.copy()\n",
    "# coefficients_[0] = 1.645\n",
    "# coefficients_[1] = 2.115\n",
    "# coefficients_[3] = 2.84\n",
    "\n",
    "# predictions = optR.predict(test_predictions, coefficients_).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "638aff418b1928a81479041abe5408b4319f54c9"
   },
   "outputs": [],
   "source": [
    "sample = pd.read_csv('../input/petfinder-adoption-prediction/test/sample_submission.csv')\n",
    "sample.AdoptionSpeed = predictions\n",
    "sample.to_csv('submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
